%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%% thesistemplate.tex version 4.01 (2023/09/21)                               %%
%% The LaTeX template file to be used with the aaltothesis.sty (version 4.00) %%
%% style file.                                                                %%
%% This package requires pdfx.sty v. 1.5.84 (2017/05/18) or newer.            %%
%%                                                                            %%
%% This is licensed under the terms of the MIT license below.                 %%
%%                                                                            %%
%% Written by Luis R.J. Costa.                                                %%
%% Currently developed at Teacher services, Learning Services of Aalto        %%
%% University by Luis R.J. Costa since May 2019.                              %%
%%                                                                            %%
%% Copyright 2017-2021 aaltothesis.cls by Luis R.J. Costa,                    %%
%% luis.costa@aalto.fi.                                                       %%
%% Copyright 2017-2018 Swedish translations in aaltothesis.cls by Elisabeth   %%
%% Nyberg and Henrik Wallén henrik.wallen@aalto.fi.                           %%
%% Finnish documentation in the template opinnatepohja.tex translated from    %%
%% the English template documentation.                                        %%
%% Copyright 2021 English template thesistemplate.tex by Luis R.J. Costa,     %%
%% Maurice Forget, Henrik Wallén.                                             %%
%% Copyright 2018-2022 Swedish template kandidatarbetsbotten.tex by Henrik    %%
%% Wallen.                                                                    %%
%%                                                                            %%
%% Permission is hereby granted, free of charge, to any person obtaining a    %%
%% copy of this software and associated documentation files (the "Software"), %%
%% to deal in the Software without restriction, including without limitation  %%
%% the rights to use, copy, modify, merge, publish, distribute, sublicense,   %%
%% and/or sell copies of the Software, and to permit persons to whom the      %%
%% Software is furnished to do so, subject to the following conditions:       %%
%% The above copyright notice and this permission notice shall be included in %%
%% all copies or substantial portions of the Software.                        %%
%% THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR %%
%% IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   %%
%% FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL    %%
%% THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER %%
%% LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING    %%
%% FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER        %%
%% DEALINGS IN THE SOFTWARE.                                                  %%
%%                                                                            %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                            %%
%%                                                                            %%
%% An example for writting your thesis using LaTeX                            %%
%% Original version and development work by Luis Costa, changes to the text   %%
%% in the Finnish template by Perttu Puska.                                   %%
%% Support for Swedish added 15092014                                         %%
%% PDF/A-b support added on 15092017                                          %%
%% PDF/A-2 support added on 24042018                                          %%
%% Layout design and typesettin changed 15072021                              %%
%%                                                                            %%
%% This example consists of the files                                         %%
%%       thesistemplate.tex (version 4.00) (for text in English)              %%
%%       opinnaytepohja.tex (version 4.00) (for text in Finnish)              %%
%%       kandidatarbetsbotten.tex (version 1.10) (for text in Swedish)        %%
%%       aaltothesis.cls                                                      %%
%%       linediagram.pdf (graphics file)                                      %%
%%       curves.pdf      (graphics file)                                      %%
%%       ledspole.jpg    (graphics file)                                      %%
%%                                                                            %%
%%                                                                            %%
%% Typeset in Linux with                                                      %%
%% pdflatex: (recommended method)                                             %%
%%             $ pdflatex thesistemplate                                      %%
%%             $ pdflatex thesistemplate                                      %%
%%                                                                            %%
%%   The result is the file thesistemplate.pdf that is PDF/A compliant, if    %%
%%   you have chosen the proper \documenclass options (see comments below)    %%
%%   and your included graphics files have no problems.                       %%
%%                                                                            %%
%%                                                                            %%
%% Explanatory comments in this example begin with the characters %%, and     %%
%% changes that the user can make with the character %                        %%
%%                                                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% WHAT is PDF/A
%%
%% PDF/A is the ISO-standardized version of the pdf. The standard's goal is to
%% ensure that he file is reproducable even after a long time. PDF/A differs
%% from pdf in that it allows only those pdf features that support long-term
%% archiving of a file. For example, PDF/A requires that all used fonts are
%% embedded in the file, whereas a normal pdf can contain only a link to the
%% fonts in the system of the reader of the file. PDF/A also requires, among
%% other things, data on colour definition and the encryption used.
%% Currently three PDF/A standards exist:
%% PDF/A-1: based on PDF 1.4, standard ISO19005-1, published in 2005.
%%          Includes all the requirements essential for long-term archiving.
%% PDF/A-2: based on PDF 1.7, standard ISO19005-2, published in 2011.
%%          In addition to the above, it supports embedding of OpenType fonts,
%%          transparency in the colour definition and digital signatures.
%% PDF/A-3: based on PDF 1.7, standard ISO19005-3, published in 2012.
%%          Differs from the above only in that it allows embedding of files in
%%          any format (e.g., xml, csv, cad, spreadsheet or wordprocessing
%%          formats) into the pdf file.
%% PDF/A-4: based on PDF 2.0, standard ISO19005-4, published in November 2020.
%%
%% PDF/A-1 files are not necessarily PDF/A-2 -compatible and PDF/A-2 are not
%% necessarily PDF/A-1 -compatible.
%% Standards PDF/A-1, PDF/A-2 and PDF/A-3 have two levels:
%% b: (basic) requires that the visual appearance of the document is reliably
%%    reproduceable.
%% a (accessible) in addition to the b-level requirements, specifies how
%%   accessible the pdf file is to assistive software, say, for the physically
%%   impaired.
%% The PDF/A-4 standard does not have additional levels like in the earlier
%% standards.
%% For more details on PDF/A, see, e.g.,
%% https://www.loc.gov/preservation/digital/formats/fdd/fdd000318.shtml or
%% https://www.pdfa.org/resource/iso-19005-pdfa/
%%
%%
%% WHICH PDF/A standard should my thesis conform to?
%%
%% Either to the PDF/A-1b or the PDF/A-2b standard. If all the figures and
%% graphs used in thesis work do not require transparency features, use either
%% PDF/A-1b or PFDF/A-2b. If you have figures with transparency
%% characteristics, use the PDF/A-2b standard. However, drawing applications
%% often use the transparency parameter, setting it to zero, to specify opacity
%% and get the basic 2-D visualisation. As a result, validation of PDF/A-1b
%% will fail. Use PDF/A-2b if PDF/A-1b validation fails.
%% Do not use the PDF/A-3b standard for your thesis.
%% The font to be used are specified in this templatenand they should not be
%% changed. In addition to not adhering to Aalto's visual guidelines, you may
%% have difficulties in producing a PDF/A-compliant PDF.
%%
%%
%% Validate your PDF/A file at https://www.pdf-online.com/osa/validate.aspx
%%
%%
%% WHAT graphics format can I use to produce my PDF/A compliant file?
%%
%% When using pdflatex to compile your work, favour the use of pdf, but you can
%% use the jpg or png format especially for photographs. You will have PDF/A
%% compliance problems with figures in pdf if the fonts are not embedded in the
%% pdf file.
%% If you choose to use latex to compile your work, the only acceptable file
%% format for your figure is eps. DO NOT use the ps format for your figures.

%% USE one of the following three \documentclass set-ups:
%% * the first when using pdflatex to directly typeset your document in the
%%   chosen pdf/a format for online publishing (centred page layout),
%% * the second for one-sided printing your thesis with the layout (wide left
%%   margin), or
%% * the third for two-sided printing.
%%
\documentclass[english, 12pt, a4paper, elec, utf8, a-2b, online]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, utf8, a-2b, print]{aaltothesis}
%\documentclass[english, 12pt, a4paper, elec, utf8, a-2b, print, twoside]{aaltothesis}

%% Use the following options in the \documentclass macro above:
%% your school: arts, biz, chem, elec, eng, sci
%% the character encoding scheme used by your editor: utf8, latin1
%% thesis language: english, finnish, swedish
%% make an archiveable PDF/A-1b or PDF/A-2b compliant file: a-1b, a-2b
%%                    (with pdflatex, a normal pdf containing metadata is
%%                     produced without the a-*b option)
%% typset for online document or print on paper: online, print
%%        online: typeset in symmetric layout and blue hypertext for online
%%                publishing
%%        print: typeset in a symmetric layout and black hypertext for printing
%%               on paper
%%          two-side printing: twoside (default is one-sided printing)
%%               typeset in a wide margin on the binding side of the page and
%%               black hypertext. Use with print only.
%%

%% Use one of these if you write in Finnish (or use the Finnish template
%% opinnaytepohja.tex)
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, online]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, print]{aaltothesis}
%\documentclass[finnish, 12pt, a4paper, elec, utf8, a-1b, print, twoside]{aaltothesis}

%% Use one of these if you write in Swedish (or use the Swedish template
%% kandidatarbetsbotten.tex)
%\documentclass[swedish, 12pt, a4paper, elec, utf8, a-2b, online]{aaltothesis}
%\documentclass[swedish, 12pt, a4paper, elec, utf8, a-2b]{aaltothesis}
%\documentclass[swedish, 12pt, a4paper, elec, dvips, online]{aaltothesis}

%% FOR USERS OF AMS PACKAGES:
%% * newtxmath used in this template loads amsmath, so
%%   you needn't load it. If you want to use options in amsmath, load it here,
%%   before \setupthesisfonts below to pass the options to amsmath.
%% * If you want to use amsthm, load it here before \setupthesisfonts to avoid
%%   a clash with newtxmath.
%% * If using amsmath with options and you want to use amsthm, load amsthms
%%   after amsmath, as described in the amsthm documentation.
%% * Don't use amsbsym or amsfonts. The symbols [and macros] there are defined in
%%   newtxmath and so clash if used.
%\usepackage[options]{amsmath}
%\usepackage{amsthm}

%% DO NOT MOVE OR REMOVE \setupthesisfonts
\setupthesisfonts

%%
%% Add here the packges you need
%%
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{minted}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{float}


\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\sisetup{
    round-mode=places,
    round-precision=2,
    scientific-notation=false
}

\usepackage[version=4]{mhchem}
% \usepackage{fontspec}


\usepackage[printonlyused,withpage]{acronym}


%% For tables that span multiple pages; used to split a paraphrasing example in
%% the appendix. If you don't need it, remove it.
\usepackage{longtable}

%% A package for generating Creative Commons copyright terms. If you don't use
%% the CC copyright terms, remove it, since otherwise undesired information may
%% be added to this document's metadata.
\usepackage[type={CC}, modifier={by-nc-sa}, version={4.0}]{doclicense}
%% Find below three examples for typesetting the CC license notice.


%% Edit to conform to your degree programme
%% Capitalise the words in the name of the degree programme: it's a name
\degreeprogram{Data Science}
%%

%% Your major
%%
\major{ICT Innovation}
%%

%% Choose one of the three below
%%
%\univdegree{BSc}
\univdegree{MSc}
%\univdegree{Lic}
%%

%% Your name (self explanatory...)
%%
\thesisauthor{Gabriel Gomes Ziegler}
%%

%% Your thesis title and possible subtitle comes here and possibly, again,
%% together with the Finnish or Swedish abstract. Do not hyphenate the title
%% (and subtitle), and avoid writing too long a title. Should LaTeX typeset a
%% long title (and/or subtitle) unsatisfactorily, you might have to force a
%% linebreak using the \\ control characters. In this case...
%% * Remember, the title should not be hyphenated!
%% * A possible 'and' in the title should not be the last word in the line; it
%%   begins the next line.
%% * Specify the title (and/or subtitle) again without the linebreak characters
%%   in the optional argument in box brackets. This is done because the title
%%   is part of the metadata in the pdf/a file, and the metadata cannot contain
%%   linebreaks.
%%
% \thesistitle{Automating Information Extraction from Non-Standard Financial Reports Using Large Language Models}
\thesistitle{Automating Information Extraction from Financial Reports Using LLMs}
%\thesistitle[Title of the thesis]{Title of\\ the thesis}
%%
%% Either remove or leave \thesissubtitle{} empty if you don't use it
%%
% \thesissubtitle{Enhancing Efficiency through Format-Aware Extraction with Large Language Models}
\thesissubtitle{A Comparative Study of Text, Image, and Multimodal Approaches}
%\thesissubtitle[Subtitle of the thesis]{Subtitle of\\ the thesis}
%\thesissubtitle{}

%%
\place{Espoo}
%%

%% The date for the bachelor's thesis is the day it is presented
%%
\date{26 September 2024}
%%

%% Thesis supervisor
%% Note the "\" character in the title after the period and before the space
%% and the following character string.
%% This is because the period is not the end of a sentence after which a
%% slightly longer space follows, but what is desired is a regular interword
%% space.
%%
\supervisor{Prof.\ Bo Zhao}
%%

%% Advisor(s)---two at the most---of the thesis. Check with your supervisor how
%% many official advisors you can have.
%%
\advisor{MS Liliya Shakhpazyan (MSc)}
%%

%% If you do your thesis work in a company of other institute, give the name of
%% the company or instution here. Otherwise, leave the macro empty, comment it
%% out, or remove it. This will remove this field from the abstract page.
%%
\collaborativepartner{Datia}
%%

%% Aaltologo: syntax:
%% \uselogo{?|!|''}
%% The logo language is set to be the same as the thesis language.
%%
%\uselogo{?}
\uselogo{''}
%\uselogo{!}
%%

%%%%%%%%%%%%%%%%%%               COPYRIGHT TEXT               %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Copyright of a work is with the creator/author of the work regardless of
%% whether the copyright mark is explicitly in the work or not. You may, if you
%% wish---we encourage you to do so---publish your work under a Creative
%% Commons license (see creativecommons.org), in which case the license text
%% must be visible in the work. Write here the copyright text you want using the
%% macro \copyrighttext, which writes the text into the metadata of the pdf file
%% as well.
%%
%% Syntax:
%% \copyrigthtext{metadata text}{text visible on the page}
%%
%% CHOOSE ONE OF THE COPYRIGHT NOTICE STYLES BELOW.
%% IF USING THE CC TERMS, CHOOSE THE LICENSE YOU WANT TO USE.
%% The different CC licenses are listed at
%% https://creativecommons.org/about/cclicenses/.
%% If you use the icons from the dolicense.sty package, add the package above
%% (\usepackage{dolicense}).
%% IMPORTANT NOTE!! Manually write the CC text in the \copyrighttext metadata
%% text field.
%%
%% NOTE: In the macros below, the text written in the metadata must have a
%% \noexpand macro before the \copyright special character. When not in pdf/a
%% mode (i.e. a-1b or a-2b are not specified in \documentclass), two \noexpands
%% are required in the metadata text to correctly render the copyright mark in
%% the pdf metadata. In pdf/a mode one \noexpand suffices.
%%
%% EXAMPLE OF PLAIN COPYRIGHT TEXT
%% The macros \copyright and \year below must be separated by the \ character
%% (space chacter) from the text that follows. The macros in the argument of the
%% \copyrighttext macro automatically insert the year and the author's name.
%% (Note! \ThesisAuthor is an internal macro of the aaltothesis.cls class file).
%%
%\copyrighttext{Copyright \noexpand\textcopyright\ \number\year\ \ThesisAuthor}
%{Copyright \textcopyright{} \number\year{} \ThesisAuthor}
%%
%% Of course, the same text could have simply been written as
%% \copyrighttext{Copyright \noexpand\copyright\ 2018 Eddie Engineer}
%% {Copyright \copyright{} 2022 Eddie Engineer}
%%
%% EXAMPLES OF CC LICENSE: different ways to display the same license
%% 1. A simple Creative Commons license text with a link to the copyright notice:
%\copyrighttext{\noexpand\textcopyright\ \number\year. This work is
%	licensed under a CC BY-NC-SA 4.0 license.}{\textcopyright{}
%	\number\year. This work is licensed under a
%	\href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{CC BY-NC-SA 4.0}
%	license.}
%
%% To get the URL of the license of your choice, go to
%% https://creativecommons.org/about/cclicenses/, click on the chosen license
%% you want to use, and copy-and-paste the URL in the macro \href above.
%%
%% 2. A short Creative Commons license text containing the respective CC icons
%% (requires the package dolicense.sty to be added in the preamble as done
%% above) and a link to the corresponding Creative Commons license webpage (see
%% the dolicense package documentation for other license icons):
%\copyrighttext{\noexpand\textcopyright\ \number\year. This work is licensed
%	under a CC BY-NC-SA 4.0 license.}{
%	\parbox{95mm}{\noindent\textcopyright\ \number\year. \doclicenseText}
%	\hspace{1em}\parbox{35mm}{\doclicenseImage}
%}
%%
%% 3. An expanded Creative Commons license text containing the respective CC
%% icons text and as generated by the dolicense.sty package (the license is set
%% via package options in \usepackage[options]{dolicense} above; see the
%% dolicense package documentation for other license texts and icons):
\copyrighttext{\noexpand\textcopyright\ \number\year. This work is
    licensed under a Creative Commons "Attribution-NonCommercial-ShareAlike 4.0
    International" (BY-NC-SA 4.0) license.}{\noindent\textcopyright\ \number
    \year \ \doclicenseThis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% The English abstract:
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above.
%% Thesis keywords:
%% Note! The keywords are separated using the \spc macro
%%
\keywords{Large Language Models\spc Document AI\spc Information Extraction\spc GPT-4\spc Deep Learning\spc Machine Learning\spc Automated Data Retrieval\spc Natural Language Processing}

%%

%% The abstract text. This text in one paragraph is included in the metadata of
%% the pdf file as well as the abstract page. To have paragraphs in your
%% abstract rewrite it in the abstarct environment as described below.
%%
\thesisabstract{%
This thesis investigates the application of the latest \ac{LLM}s for the automated extraction of \ac{ESG} indicators from financial reports, a critical task for companies focused on keeping up-to-date sustainability reporting.
The study explores the trade-offs in three distinct approaches: text-only, image-only, and multimodal, to evaluate their effectiveness in a real-world diverse dataset of financial reports.
The text-only approach, although effective for documents with structured textual data, struggled with visual-rich content, leading to high residual difference between the extracted and actual values.
The image-only approach, while adept at interpreting visual elements, faced challenges with hallucinations and lacked the accuracy of text-based methods that tend to get more perfect matches than the image-based counterparts.
The multimodal approach, which combines text and image data, demonstrated superior performance, achieving the highest accuracy with a perfect match rate exceeding 85\% for key indicators like Scope 1 and Scope 3 emissions.
This method effectively mitigated errors through cross-validation between text and image data, resulting in minimal residuals and reliable data extraction.
The study's findings underscore the possibility of using the three approaches according to the nature of the dataset having a more confident usage when applying the combined multimodal approach in DocumentAI, particularly in format-agnostic scenarios such as the one presented in this study.
The thesis also identifies challenges, such as the ambiguity in sub-indicators under Scope 2 emissions, and the importance of examining the tolerance for errors in the context of the application, where residuals can be acceptable or make the system completely unusable.
This work contributes to the growing field of DocumentAI, offering insights into the capabilities and limitations of \ac{LLM} for financial report analysis, and suggests pathways for future advancements in automated information retrieval systems.
}

%% You can prevent LaTeX from writing into the xmpdata file (it contains all the
%% metadata to be written into the pdf file) by setting the writexmpdata switch
%% to 'false'. This allows you to write the metadata in the correct format
%% directly into the file thesistemplate.xmpdata.
%\setboolean{writexmpdatafile}{false}


%% All that is printed on paper starts here
%%
\begin{document}

%% Create the coverpage
%%
\makecoverpage

%% Typeset the copyright text.
%% If you wish, you may leave out the copyright text from the human-readable
%% page of the pdf file. This may seem like a attractive idea for the printed
%% document especially if "Copyright (c) yyyy Eddie Engineer" is the only text
%% on the page. However, the recommendation is to print this copyright text.
%%
\makecopyrightpage

\clearpage
%% Note that when writing your thesis in English, place the English abstract
%% first followed by the possible Finnish or Swedish abstract.

%% Abstract text
%% All the details (name, title, etc.) on the abstract page appear as specified
%% above. Add your abstarct text with paragraphs here to have paragraphs in the
%% visible abstract page. Nonetheless, write the abstarct text without
%% paragraphs in the macro \thesismacro so that it is added to the metadata as
%% well.
%%
\begin{abstractpage}[english]
This thesis investigates the application of the latest \ac{LLM}s for the automated extraction of \ac{ESG} indicators from financial reports, a critical task for companies focused on keeping up-to-date sustainability reporting.
The study explores the trade-offs in three distinct approaches: text-only, image-only, and multimodal, to evaluate their effectiveness in a real-world diverse dataset of financial reports.

The text-only approach, although effective for documents with structured textual data, struggled with visual-rich content, leading to high residual difference between the extracted and actual values.
The image-only approach, while adept at interpreting visual elements, faced challenges with hallucinations and lacked the accuracy of text-based methods that tend to get more perfect matches than the image-based counterparts.
The multimodal approach, which combines text and image data, demonstrated superior performance, achieving the highest accuracy with a perfect match rate exceeding 85\% for key indicators like Scope 1 and Scope 3 emissions.
This method effectively mitigated errors through cross-validation between text and image data, resulting in minimal residuals and reliable data extraction.

The study's findings underscore the possibility of using the three approaches according to the nature of the dataset having a more confident usage when applying the combined multimodal approach in DocumentAI, particularly in format-agnostic scenarios such as the one presented in this study.
The thesis also identifies challenges, such as the ambiguity in sub-indicators under Scope 2 emissions, and the importance of examining the tolerance for errors in the context of the application, where residuals can be acceptable or make the system completely unusable.
This work contributes to the growing field of DocumentAI, offering insights into the capabilities and limitations of \ac{LLM} for financial report analysis, and suggests pathways for future advancements in automated information retrieval systems.
\end{abstractpage}

%% The text in the \thesisabstract macro is stored in the macro \abstractext, so
%% you can use the text metadata abstract directly as follows:
%%
%\begin{abstractpage}[english]
%	\abstracttext{}
%\end{abstractpage}

\begin{abstractpage}[finnish]
\end{abstractpage}

\dothesispagenumbering{}

%% Preface
%%
%% This section is optional. Remove it if you do not want a preface.
\mysection{Preface}
%\mysection{Esipuhe}

Thanks notes

\vspace{5cm}
Otaniemi, 26 September 2024\\

\vspace{5mm}
{\hfill Gabriel Gomes Ziegler \hspace{1cm}}

%% Force a new page after the preface
%%
\newpage


%% Table of contents.
%%
\thesistableofcontents

\cleardoublepage

\newpage

\acrodef{AI}[AI]{Artificial Intelligence}
\acrodef{ML}[ML]{Machine Learning}
\acrodef{DL}[DL]{Deep Learning}
\acrodef{NLP}[NLP]{Natural Language Processing}
\acrodef{CV}[CV]{Computer Vision}
\acrodef{ANN}[ANN]{Artificial Neural Networks}
\acrodef{CNN}[CNN]{Convolutional Neural Networks}
\acrodef{RNN}[RNN]{Recurrent Neural Networks}
\acrodef{LSTM}[LSTM]{Long Short-Term Memory}
\acrodef{PDF}[PDF]{Portable Document Format}
\acrodef{OCR}[OCR]{Optical Character Recognition}
\acrodef{LLM}[LLM]{Large Language Model}
\acrodef{GPT}[GPT]{Generative Pre-trained Transformer}
\acrodef{BERT}[BERT]{Bidirectional Encoder Representations from Transformers}
\acrodef{KPI}[KPI]{Key Performance Indicator}
\acrodef{RAG}[RAG]{Retrieval Augmented Generation}
\acrodef{ESG}[ESG]{Environmental, Social, and Governance}
\acrodef{GHG}[GHG]{Greenhouse Gas}
\acrodef{JSON}[JSON]{JavaScript Object Notation}
\acrodef{MAPE}[MAPE]{Mean Absolute Percentage Error}
\acrodef{DPI}[DPI]{Dots Per Inch}
\acrodef{RLHF}[RLHF]{Reinforcement Learning from Human Feedback}
\acrodef{IR}[IR]{Information Retrieval}

\begin{acronym}
    \acro{AI}{Artificial Intelligence}
    \acro{ML}{Machine Learning}
    \acro{DL}{Deep Learning}
    \acro{NLP}{Natural Language Processing}
    \acro{CV}{Computer Vision}
    \acro{ANN}{Artificial Neural Networks}
    \acro{CNN}{Convolutional Neural Networks}
    \acro{RNN}{Recurrent Neural Networks}
    \acro{LSTM}{Long Short-Term Memory}
    \acro{PDF}{Portable Document Format}
    \acro{OCR}{Optical Character Recognition}
    \acro{LLM}{Large Language Model}
    \acro{GPT}{Generative Pre-trained Transformer}
    \acro{BERT}{Bidirectional Encoder Representations from Transformers}
    \acro{KPI}{Key Performance Indicator}
    \acro{RAG}{Retrieval Augmented Generation}
    \acro{ESG}{Environmental, Social, and Governance}
    \acro{GHG}{Greenhouse Gas}
    \acro{JSON}{JavaScript Object Notation}
    \acro{MAPE}{Mean Absolute Percentage Error}
    \acro{DPI}{Dots Per Inch}
    \acro{RLHF}{Reinforcement Learning from Human Feedback}
    \acro{IR}{Information Retrieval}
\end{acronym}

%% Text body begins. Note that since the text body is mostly in Finnish the
%% majority of comments are also in Finnish after this point. There is no point
%% in explaining Finnish-language specific thesis conventions in English.
%% This text will be translated to English soon.
%%


%% Leave page number of the first page empty
%%
\thispagestyle{empty}

\newpage
\section{Introduction}
\label{sec:intro}

\subsection{Overview}

The digital age has transformed how information is stored, accessed, and analyzed, with financial reports being a critical source of data for businesses, regulators, and investors.
Traditionally, extracting meaningful information from these reports required manual labor or rule-based systems, which were not only time-consuming but also prone to errors.
The advent of machine learning, particularly the development of large language models (\acp{LLM}), has brought about significant advancements in this field.
These models, such as \ac{GPT} and \ac{BERT}, offer unprecedented capabilities in processing unstructured data, making it possible to automate some information extraction tasks with high accuracy and efficiency.

\subsection{Context and Motivation}

Financial reports are rich sources of information, encompassing a wide range of data including financial statements, management discussions, and \ac{ESG} disclosures.
Among these, the extraction of \ac{GHG} emissions data has gained particular importance due to increasing regulatory requirements and the growing emphasis on sustainability.
However, the diverse formats and layouts used in these reports pose significant challenges for automated data extraction systems.
The need for a robust solution that can handle such variability is more pressing than ever as businesses and stakeholders demand faster and more accurate insights from financial documents.

\subsection{Problem Definition}

The core challenge addressed in this thesis is the development and comparison of methodologies for extracting \ac{GHG} emissions data from non-standardized financial reports.
This problem is compounded by the variety of ways in which companies present their data, ranging from structured tables to complex visual elements such as charts and infographics.
Traditional approaches, whether manual or rule-based, often fall short in dealing with this complexity, leading to incomplete or inaccurate data extraction.
The rise of \acp{LLM} offers a promising avenue to overcome these limitations by leveraging advanced text and image processing capabilities.

\subsection{Objectives of the Study}

The primary objective of this study is to conduct a comprehensive evaluation of different \ac{LLM}-based approaches for extracting information from financial reports, particularly focusing on non-standard report formats.

The study aims to:
\begin{enumerate}
\item Compare the effectiveness of text-only, image-only, and multimodal approaches in extracting \ac{GHG} emissions data.
\item Enhance the precision and efficiency of data extraction by optimizing \ac{LLM} techniques for handling diverse document layouts.
\item Develop a scalable system capable of processing large volumes of financial reports while maintaining high accuracy.
\item Identify the limitations and challenges associated with current \ac{LLM} technologies in the context of financial data extraction.
\end{enumerate}

By achieving these objectives, this thesis seeks to contribute to the broader field of automated document analysis and provide actionable insights for improving data extraction processes in real-world applications.

\subsection{Research Questions}

The central research question guiding this thesis is: \textit{``What are the most effective strategies for utilizing \acp{LLM} to accurately and efficiently extract financial data from unstructured reports?''} To address this question, several sub-questions are explored:
\begin{itemize}
\item What \ac{LLM} architectures are most effective for recognizing and extracting financial data?
\item How can context-aware data extraction methodologies be developed to improve the accuracy of \ac{GHG} emissions data extraction?
\item What are the best practices for handling the diverse formats and layouts of financial reports?
\item How does the quality and volume of training data impact the performance of \ac{LLM}-based extraction systems?
\end{itemize}

These research questions are essential for developing a comprehensive understanding of how \acp{LLM} can be effectively applied to the task of financial data extraction, particularly in the context of non-standard report formats.

\subsection{Scope and Limitations}

This study is focused on the extraction of financial data, specifically \ac{GHG} emissions metrics, from financial reports.
The reports used in this study are publicly available and include annual and quarterly statements from a variety of industries.
The scope is deliberately confined to \ac{GHG} emissions data to maintain a clear focus, although the methodologies developed could be extended to other types of financial data in future research.
The primary constraints include the variability in report formats, the complexity of financial and environmental terminology, and the current limitations of \ac{LLM} technologies in fully understanding and processing domain-specific contexts.
Additionally, the study is limited by the computational resources available, which may affect the scalability and speed of the proposed solutions.

\subsection{Structure of the Thesis}

This thesis is organized to systematically explore and address the challenges of financial data extraction using \acp{LLM}.
The structure is as follows:
\begin{enumerate}
\item \textbf{Introduction}: Provides the context, problem definition, and objectives of the study.
\item \textbf{Literature Review}: Discusses the concepts and state of the art in document analysis, \ac{LLM}s, and information retrieval from financial reports.
\item \textbf{Methodology}: Details the dataset used, the experimental setup, and the methodologies implemented for data extraction.
\item \textbf{Results}: Presents and analyzes the results of the experiments, comparing the performance of different approaches.
\item \textbf{Conclusion}: Summarizes the findings, discusses the implications and limitations, and suggests directions for future research.
\item \textbf{References}: Lists the academic and technical sources cited throughout the thesis.
\end{enumerate}

This structure is designed to provide a logical flow of information, guiding the reader through the development, implementation, and evaluation of the research conducted in this thesis.

\clearpage
\section{Concepts and State of the Art}

Documenting and searching for information is an ancient human practice that can be traced back as far as 3000 BC, when the Sumerians—the first civilization in the world—used clay tablets with cuneiform inscriptions to keep track of legal documents, transaction records, literature, mythological tales, among other information.
They also created different categories to differentiate tablets based on their contents in a classification fashion \cite{finkel2015cuneiform}.
Similar practices have remained largely relevant throughout history.
With the invention of paper and the printing press, the practice of documenting and storing information evolved, allowing for more information to be stored and shared physically.
Not so long ago, in the 20th century, the invention of the computer and the internet revolutionized the way information is stored and shared, allowing for information to be stored and shared digitally.
This allowed for vast amounts of information to be stored and shared in a way that was never possible before.
This period in time is often referred to as the information age, also known as the third industrial revolution, which marks a time when information became increasingly accessible and also a commodity, especially later in the 21st century with the rise of \ac{AI} and \ac{ML} models that feed on large datasets to learn and make predictions.
Additionally, the creation of \ac{PDF}s by Adobe in 1993 \cite{adobe2023pdftimeline} established a standard for storing and sharing information in a portable format that could be easily shared and printed, quickly becoming a standard for sharing information, especially in the business world.
The digitalized information stored in \ac{PDF}s soon became a target for information retrieval and data mining techniques, where systems were developed to extract information from these files based on a variety of approaches, such as heuristic-based methods \cite{table_extraction_conditional_pinto_2003, PENG2006963, Fang2004}, vector space models \cite{Salton1975, Wong1987}, probabilistic models \cite{Maron1960}, and more recently, \ac{DL} models, especially those leveraging the transformer architecture \cite{Wang2021, Li2022, Li2023, Xu2020}.

\subsection{Information Retrieval}

\ac{IR} is a field of study that focuses on the organization, storage, and retrieval of bibliographic information.
\ac{IR} systems are used to provide a response to a user query with references to documents that contain the information sought by the user.
Although the field of \ac{IR} has been improved and become more popular in recent years with the usage of \ac{NLP} techniques dominating the field, such as \ac{RAG}s, the core task of \ac{IR} has been studied and applied since the 1940s, with pioneers like Vannevar Bush, who in 1945 envisioned the Memex, a theoretical device that would store extensive collections of documents and allow rapid retrieval and cross-referencing of information \cite{Bush1945As}.

\subsection{Document AI}

The procedure of extracting information from a document—recently referred to as ``Document AI'' \cite{Cui2021} or ``Document Understanding'' \cite{Nishant2020}—is a complex problem due to the diverse nature of data that \ac{PDF}s allow to store.
Such problems often involve cross-modal interactions where information is represented in both natural language text and visual elements such as tables, charts, and images.
In the visual domain, document layout analysis has been widely studied and applied using \ac{CV} techniques to detect and extract elements in the document.
Document image processing is usually treated as an object detection task where elements such as text, tables, and images are detected and classified \cite{Yang2017, Schreiber2017}.

In many scenarios, the information presented in a document requires an approach that can leverage understanding from both the text and visual domains.
For instance, consider tables where the values in cells only have meaning when associated with the header row, which is commonly described in the first row or column of the table.
Other structures such as charts only convey information when the labels and axes are correctly identified and associated with the data points.
These problems are particularly true for financial reports, where information is presented in text, tables, charts, infographics, and other visual elements.
The requirement to handle layout invariance is tackled by general-purpose models, such as \ac{LLM}s, that through pre-training of general-purpose models, learn the position information of the elements as well as the visual information presented with the text, such as font size, color, and style.
These visual cues can be learned by visual encoders and combined with the text in the pre-training stage, significantly improving the model's capability to abstract non-standardized information from documents \cite{Cui2021}.

\subsection{Transformers}

In recent years, \ac{AI} systems with \ac{DL} architectures have been the norm for most of the state-of-the-art models introduced.
The first \ac{DL} models were based on fully-connected neural networks, which are composed of layers of neurons that are connected to each other in a feed-forward fashion \cite{Rosenblatt1958, Rosenblatt1961}.
Decades after the first fully-connected neural networks were published, the \ac{CNN} architecture was proposed by Yann LeCun in 1998 \cite{LeCun1998} and later popularized by the AlexNet model in 2012 after winning the ImageNet competition \cite{Krizhevsky2012}.
These models represented a significant improvement in the field of \ac{CV} and were able to learn hierarchical features from images, allowing for more complex patterns to be learned.
Meanwhile, in the field of \ac{NLP}, the \ac{RNN} architecture was proposed in 1986, bringing the concept of sequential processing to neural networks, allowing previous states to be considered in the processing of the current state \cite{Rumelhart1986}.
The \ac{RNN} architecture was able to learn sequential patterns from text data, allowing for the development of models that could generate text, translate languages, and more.
However, the \ac{RNN} architecture had limitations in learning long-range dependencies due to the vanishing gradient problem \cite{Hochreiter1991}.
To address this issue, the \ac{LSTM} architecture was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber \cite{Hochreiter1997}.
The \ac{LSTM} architecture introduced the concept of memory cells that could store information over time, allowing for the learning of long-range dependencies in sequential data.
The next breakthrough in the \ac{AI} field came with the introduction of the transformer architecture in 2017, which revolutionized the field of \ac{NLP} by utilizing self-attention mechanisms to learn intrinsic features \cite{Vaswani2017}.
The transformer architecture brought significant improvements in the field of \ac{NLP}, as demonstrated by the BERT model in 2019 \cite{Devlin2019} and later by the GPT-3 model in 2020 \cite{Brown2020}, which would become the state-of-the-art models in the field of \ac{NLP}.
Much inspired by the major advancements in the field of \ac{NLP} with the transformer architecture, researchers started applying the transformer architecture to the field of \ac{CV} as an alternative to the long-standing \ac{CNN} architecture that has been the standard for many years \cite{Chen2021}.
These models, known as visual transformers, have shown to be competitive with \ac{CNN}s in many tasks and have demonstrated the potential to learn hierarchical features from images in a more efficient way than \ac{CNN}s \cite{Dosovitskiy2021}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/transformers_milestones.png}
    \caption{Key milestones in the development of transformer.
    The vision transformer models are marked in red (Image from `A Survey on Vision Transformer' \cite{Han2023}).}
    \label{fig:transformers_milestones}
\end{figure}

The original transformer architecture \cite{Vaswani2017} is a sequence-to-sequence model composed of an encoder and a decoder block that are themselves composed of multiple layers of multi-head self-attention mechanisms and feed-forward neural networks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/transformer_architecture.png}
    \caption{Transformer architecture.
    The model is composed of an encoder and a decoder block that are themselves composed of multiple layers of multi-head self-attention mechanisms and feed-forward neural networks \cite{Vaswani2017}.}
    \label{fig:transformers_architecture}
\end{figure}

The attention function, often described as a map of a query and a set of key-value pairs to an output, outputs a weighted sum of the values based on the similarity of the query to the keys, computing a compatibility function between the query and the corresponding key.

\begin{figure}[H]
    \centering
    \begin{minipage}{.4\textwidth}
        \centering
        \includegraphics[height=4cm]{images/attention_qkv.png}
        \caption{Scaled dot-product attention function \cite{Vaswani2017}.}
        \label{fig:attention_qkv}
    \end{minipage}%
    \begin{minipage}{.4\textwidth}
        \centering
        \includegraphics[height=4cm]{images/multihead_attention.png}
        \caption{Multi-head attention mechanism \cite{Vaswani2017}.}
        \label{fig:multihead_attention}
    \end{minipage}
\end{figure}

The attention function is computed on a set of queries in a parallel fashion, where the queries are packed into a matrix $Q$, the keys into a matrix $K$, and the values into a matrix $V$.
The attention function is computed as follows \cite{Vaswani2017}:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\subsubsection{Transformers in Vision}

Before the introduction of the transformer architecture in the visual domain, traditional supervised techniques suffered from the necessity of large amounts of labeled data to learn hierarchical features from images, limiting the scalability and generalization of such models.
The incorporation of models such as BERT, GPT, and DALL-E, which are trained on large amounts of text data and generally with a self-supervised learning approach, leads to a decreased dependence on carefully annotated labels, which might allow room for progress on essential cognitive skills that have been challenging in the traditional supervised learning paradigm \cite{Bommasani2021, Zellers2019, Martin2023}.

The application of transformer architectures to vision tasks has revolutionized the field.
Vision Transformers (ViTs) have demonstrated that by treating image patches as sequences of tokens, similar to words in text, one can leverage the powerful sequence modeling capabilities of transformers to process and analyze visual data.
ViTs have shown remarkable performance across various tasks, including image classification, object detection, and segmentation, often surpassing the performance of traditional \ac{CNN}s \cite{Dosovitskiy2021}.

With such a trend of transformer architectures in the visual domain, OpenAI released GPT-4 Vision, which integrates advanced transformer architectures specifically designed for understanding and generating visual content.
GPT-4 Vision builds on the principles of its text-based equivalent GPT-4 but adapts the transformer model to handle the unique challenges of image inputs \cite{2023GPT4VisionSC}.
This includes learning complex spatial relationships and capturing fine-grained details from images, which are crucial for tasks such as image synthesis, enhancement, and interpretation.

The integration of multimodal learning in GPT-4 Vision allows for the processing of textual and visual information, enhancing the model's ability to understand and generate coherent and contextually enriched content.
This multimodal capability is particularly valuable in processing documents that contain both text and visual elements, such as financial reports, where information is presented in a variety of formats, including tables, charts, and images.
For these reasons and for its availability, GPT-4 Vision is the model of choice for multimodality experiments in this study.

\subsection{Large Language Models}

\ac{LLM}s are a class of statistical language models based on neural networks—oftentimes utilizing the transformer architecture—that are large in size and pre-trained on vast amounts of text data.
These models represent a significant advancement in the \ac{NLP} field, allowing complex tasks to be performed with high accuracy and efficiency.
This technology has been widely researched and developed by leading big tech companies such as OpenAI, Google, and Meta \cite{Brown2020, Devlin2019, Radford2021, OpenAI2023GPT4, Gemini2022, Gemini2024}, which have pushed the boundaries of transformer pre-trained models to the point that they can generate human-like text, understand context, and even perform tasks that require domain-specific knowledge.
This milestone signifies not only a significant improvement in the field of \ac{NLP} but also a major adoption of artificial intelligence agents by the average user, especially with OpenAI's ChatGPT release in 2022, which delivered a fine-tuned GPT-3.5 model utilizing reinforcement learning from human feedback using the same methods as the InstructGPT model \cite{Ouyang2022}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/llms_milestones.png}
    \caption{Key milestones of large language models from the task-solving perspective \cite{Zhao2023}.}
    \label{fig:llms_milestones}
\end{figure}

\subsubsection{GPT-4}

The fourth \ac{GPT} release by OpenAI is a large language model with multilingual and multimodal capabilities that allow it to process image and text inputs, producing text outputs.
The model was developed with the aim of improving the ability to comprehend and generate natural language text.
\ac{GPT}-4 is often evaluated against human performance on tasks like the bar exam, LSAT, SAT, among others, and has shown to be competitive with human performance by achieving top 10\% scores on bar exams, compared to \ac{GPT}-3.5, which achieved bottom 10\% scores \cite{OpenAI2023GPT4}.
The specifics about the model's architecture and training data are not disclosed by OpenAI, but it is known that the model has been pre-trained to predict the next word in a sentence, using publicly available and licensed data fine-tuned with \ac{RLHF}, which has a great impact on the model's performance \cite{OpenAI2023GPT4}.
While this new model has shown significant improvements in language understanding, generation, and reduction of hallucinations, it still has limitations in understanding context and generating coherent responses, which is a common issue in large language models \cite{OpenAI2023GPT4}.

\subsubsection{GPT-4V}

GPT-4 Vision represents an extension of the capabilities of traditional \ac{LLM}s into the realm of visual understanding and analysis.
By integrating vision-based artificial intelligence technologies with the language processing prowess of GPT-4, this model can interpret and analyze images, diagrams, and visual data in conjunction with textual information.
This multimodal approach enables GPT-4 Vision to perform tasks that require an understanding of both visual and textual content, such as extracting data from charts and graphs in financial reports, identifying key information in documents with complex layouts, and answering questions that depend on visual cues.
The development of GPT-4 Vision is a testament to the ongoing advancements in \ac{AI}, highlighting the move towards more integrated and comprehensive models that can navigate the complexities of human communication and information processing \cite{2023GPT4VisionSC}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/gpt_exam_results.png}
    \caption{}
    \label{fig:gpt_exam_results}
\end{figure}

\subsection{LLMs for Document AI}

\ac{LLM}s have become a popular strategy in the field of Document \ac{AI}, transforming how information is extracted, processed, and analyzed from documents.
In the context of Document \ac{AI}, \ac{LLM}s are utilized to understand the content within documents, ranging from simple text to complex structures like tables and charts, and the relationships between different pieces of information.
These models leverage their extensive training on diverse datasets to adapt to the specific challenges posed by document analysis, such as varying formats, layouts, and the integration of multimodal data.
Through techniques such as transfer learning and fine-tuning, \ac{LLM}s can be specialized to perform tasks including, but not limited to, information extraction, document summarization, and semantic search within documents.
Their ability to process and analyze documents at scale significantly reduces the time and effort required for data entry, extraction, and analysis, enabling more efficient and accurate handling of document-based information.

\subsection{Question Answering with RAG}

\ac{RAG} represents a novel approach in leveraging \ac{LLM}s for the task of question answering.
\ac{RAG} combines the generative capabilities of GPT-like models with retrieval-based methods, which search a large corpus of documents to find relevant information that can aid in generating accurate and informative answers.
This technique involves two main components: a retriever, which identifies relevant documents or passages given a query, and a generator, which synthesizes the retrieved information into a coherent response.
By integrating these two processes, \ac{RAG} is able to produce answers that are not only contextually relevant but also enriched with details and insights drawn from a wide range of sources.
This method has shown significant promise in improving the accuracy and depth of responses provided by \ac{AI} systems in question answering applications, particularly in domains where detailed and specific knowledge is required, such as academic research and technical support.

\subsection{Issues with LLMs for Document AI}

\subsubsection{Hallucinations}

Despite their many advantages, \ac{LLM}s have known limitations when applied to Document \ac{AI} tasks.
One issue that arises with the generative nature of these models is the potential for generating incorrect or misleading information, especially when the input data is ambiguous or incomplete.
These mistakes—oftentimes referred to as hallucinations—occur when the generative model creates plausible and convincing responses that are incorrect.
Although it is possible to identify and mitigate these so-called hallucinations, studies have shown via learning theory that these mistakes are inherent to the generative nature of \ac{LLM}s and cannot be completely extinguished \cite{xu2024hallucination}.
The fact that these mistakes are realistic increases the difficulty of detecting them and brings uncertainty about the reliability of the information provided by the model in a productive environment.
Comprehensive studies have been conducted to understand the causes of hallucinations and have found that these stem from a variety of reasons, including noisy data, poor parametric choices, incorrect attention mechanisms, improper training procedures, among others.
There are two distinct categories of hallucinations identified in the literature: intrinsic hallucination and extrinsic hallucination, and they require different strategies to be mitigated \cite{survey_hallucination_natural_language_generation}.

Consider the following source data used as input to an \ac{LLM} model:

\begin{quote}
    \textit{The company reported revenues of \$1 million in Q1 2022, and \$2 million in Q2 2022.}
\end{quote}

\begin{itemize}
    \label{list:hallucinations:1}
    \item \textbf{Intrinsic hallucination}: This type of hallucination occurs when the model generates information that contradicts the input data.
    A case of intrinsic hallucination would be if the model generated the following output:
    \begin{quote}
        \textit{The company reported revenues of \$1 million in Q1 2022, and \$3 million in Q2 2022.}
    \end{quote}
    Here, the model has generated information that is inconsistent with the input data since the revenue reported in Q2 2022 is incorrect according to the source data.
    This type of hallucination can be particularly problematic in document analysis and is the one that this study delves into most deeply.
    \item \textbf{Extrinsic hallucination}: Extrinsic hallucination occurs when the model generates incorrect information that is not present in the input data but is plausible given the context.
    For example, if the model generated the following output:
    \begin{quote}
        \textit{The company is projected to report revenues of \$3 million in Q3 2022.}
    \end{quote}
    This information is not present in the input data and cannot be inferred from the given context, therefore it is classified as an extrinsic hallucination.
\end{itemize}

Several techniques aim to mitigate these issues, such as using retrieval-based methods \cite{lewis2020retrievalaugmented}, fine-tuning a model on a specific domain \cite{Brown2020}, or retrying the generation process multiple times while validating the output against a defined model, as proposed by tools like \textit{LangChain} \cite{Chase_LangChain_2022} and \textit{instructor} \cite{Instructor2023}.
In this study, we show applications of how using defined models with \textit{pydantic} \cite{pydantic} and \textit{instructor} can help mitigate hallucinations in the context of document data extraction.

\subsubsection{Interpretability and Explainability}

Ever since \ac{DL} models gained popularity in productive environments, many concerns have been raised regarding the interpretability and explainability of these models, particularly in high-stakes applications such as healthcare, finance, and law, where accountability and transparency are crucial, and these models can have societal impacts, e.g., inequity, discrimination, misuse, economic and environmental impact, ethical concerns, among others.
As already demonstrated in several studies, \ac{DL} models are often treated as black boxes, because of the complex and non-linear nature of their architectures, making it difficult to understand how they arrive at their decisions and generate their outputs \cite{castelvecchi2016can, 2018_survey_explaining_black_box}.
\ac{LLM}s are no exception to this, as their complex attention mechanisms and deep learning architectures make it challenging to interpret the reasoning behind their predictions and the information they generate.

The lack of interpretability and explainability can be a significant barrier to the adoption of \ac{LLM}s in critical applications, as it not only raises concerns about the reliability, trustworthiness, and ethical implications of the decisions made by these models but also increases complexity when debugging and improving the models.
The uncertainty involving \ac{LLM}s' outputs poses a challenge for users who need to understand what kinds of inputs lead to incorrect outputs.
This is particularly important in the context of this study, as the information extracted from financial reports is used to make critical business decisions, and the reliability and accuracy of the extracted data are paramount.
For these reasons, we investigate nuances in the documents that lead to erroneous predictions so that they can be avoided in the future.

\clearpage

\section{Financial Reports Dataset}
\label{sec:dataset}

The dataset used in this study consists of a collection of annual and quarterly reports from various public companies across different industries and countries.
We have a total of 1,000 reports—comprising over 32,580 pages across all documents in the dataset—carefully annotated manually with the \ac{GHG} emissions data reported by the companies for the specified periods.
These reports contain a wide range of information, including financial statements, management discussions and analysis, auditor reports, and \ac{ESG} disclosures, including \ac{GHG} emissions data, which is the primary focus of this study.
The documents are stored in \ac{PDF} format, with varying layouts, fonts, structures, and number of pages.

We opted to create a diverse dataset with heterogeneous formats to report \ac{GHG} emission data, including text, tables, charts, and images, to evaluate the effectiveness of different strategies for extracting information from financial reports.
This approach reflects the realistic nature of the data that the system would encounter in a real-world scenario in productive environments.
Below, we present a few samples to illustrate the diversity of formats and layouts used to report \ac{GHG} emissions data in financial reports.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/ghg_emissions_bar_chart.png}
    \caption{Document reporting \ac{GHG} emissions data using a bar chart figure.}
    \label{ghg_emissions_bar_chart}
\end{figure}

The figure above shows a case where the report presents the \ac{GHG} emissions data using a bar chart.
Although this is visually rich and easy to comprehend for the human eye, it poses a significant challenge for an \ac{OCR} system to extract the data from the chart.
The text may be detected and extracted correctly by the \ac{OCR} system, but the context of the data will likely be lost, leading to an extraction with high uncertainty.
A visual or multimodal approach would be necessary to extract the data from this type of document.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/ghg_emissions_pie_chart.png}
    \caption{Document reporting \ac{GHG} emissions data using a pie chart figure.}
    \label{ghg_emissions_pie_chart}
\end{figure}

Similar to figure \ref{ghg_emissions_bar_chart}, the figure above shows a case where the report presents the \ac{GHG} emissions data using a pie chart.
In this case, since we would only be interested in the values presented in the text under the image, the \ac{OCR}-based approach could lead to a successful extraction of the data.
However, this is another example that illustrates the diversity of formats companies use to report key information in their financial reports.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/ghg_emissions_table.png}
    \caption{Document reporting \ac{GHG} emissions data in a table.}
    \label{ghg_emissions_table}
\end{figure}

One of the most common layouts present in the dataset is the table format.
It is an effective way to present structured data over a period in a concise, objective, and easy-to-read manner.
This is also one of the best formats for an \ac{OCR}-based approach to extract the data, as the data is already structured, reducing the likelihood of context loss.
However, \ac{OCR}s are not error-free with tables, especially in cases where the table is structured in a non-conventional way, such as merged cells, rotated text, or cells with no borders.
Additionally, Document \ac{AI} researchers have extensively studied and developed models to detect, extract, and understand tables in documents \cite{TableNet2020, table_extraction_conditional_pinto_2003, Schreiber2017, Li2022, Gilani2017, Shafait2010}.

\subsection{Additional Relevant Data Intricacies}

\subsubsection{Unit of Measurement}

The diversity in such documents is not only evident in the layouts but also in the selection of units reported.
Most companies disclose their \ac{GHG} emissions data in \textit{metric tons of \ce{CO2}} equivalent.
However, other companies report their emissions in different units, such as kilograms of \ce{CO2}, metric tons of \ce{CO2} per unit of production, and some companies with very large numbers report their emissions in million metric tons of \ce{CO2} equivalent.
Since the unit of measurement is crucial for the correct interpretation of the data, it is important to also be able to extract this information from the reports.
Therefore, if our system correctly detects the values in the document but fails to output a normalized unit of measurement—in our case, we use metric tonnes—the system output will be considered incorrect and will likely lead to a large residual error since the values will be in different magnitudes.

\subsubsection{Scope 2 Emissions Origin}

The \ac{GHG} Scope 2 emissions are divided into three categories: location-based, market-based, and undefined.
Location-based emissions are calculated based on the location of the company's operations, market-based emissions are calculated based on the market where the company sells its products, and undefined emissions are those not clearly defined as location-based or market-based.
It is important to be able to extract this information from the reports, as it is crucial for the correct interpretation of the data.
Therefore, if our system correctly detects the values in the document but fails to output the correct category for the Scope 2 emissions, the system output will be considered incorrect.

\clearpage

\section{Strategies for Information Extraction from Financial Reports}

Different strategies for extracting information from financial reports have been implemented to compare their effectiveness across diverse report formats and content types.
The study brings an approach focused solely on text information, an image-analysis approach that treats the \ac{PDF} page as an image, and a multimodal approach that combines image and text information to infer the correct indicators disclosed in the document from more than one source of truth.
For these experiments, we maintain the same pre-processing steps to ensure that the comparison only takes into account the core feature of extracting data from a given input.

\subsection{System Specifications}

The experiments proposed in this study were conducted on a machine with the following specifications:

\begin{itemize}
    \item \textbf{CPU}: AMD Ryzen 7 3700X 16 threads at 3.600 GHz
    \item \textbf{Memory}: 32GB at 3200 MHz
    \item \textbf{Operating System}: Linux Manjaro 6.1.55-1
    \item \textbf{Python}: 3.11.5
\end{itemize}

\subsection{Experiments Definition}

To set up an information retrieval challenge relevant to our business case and provide an opportunity to compare different strategies for extracting information from financial reports, we establish a simple set of indicators of interest, a desired schema for the extracted data, and a set of metrics to evaluate the performance of the different strategies.

\subsubsection{Indicators of Interest}

Given Datia's strong presence in the \ac{ESG} domain, we have chosen to focus on a set of \ac{ESG} indicators commonly reported in financial statements.
Therefore, this challenge focuses on correctly extracting values for the following indicators:

\begin{itemize}
    \item \textbf{\ac{GHG} Scope 1 emissions}: The total amount of \ac{GHG} emissions directly produced by a company.
    \item \textbf{\ac{GHG} Scope 2 emissions}: The total amount of \ac{GHG} emissions indirectly produced by a company.
            \begin{itemize}
                \item \textbf{Location-based}: Emissions calculated based on the location of the company's operations.
                \item \textbf{Market-based}: Emissions calculated based on the market where the company sells its products.
                \item \textbf{Undefined}: Emissions that are not clearly defined as location-based or market-based.
            \end{itemize}
    \item \textbf{\ac{GHG} Scope 3 emissions}: The total amount of \ac{GHG} emissions produced in the value chain of a company.
    Scope 3 emissions can also be broken down into 3.1, 3.3, 3.4, 3.6, 3.7, and 3.11 categories.
    However, for the purpose of this study, we will only consider the total Scope 3 emissions.
    \item \textbf{Reported Unit}: The unit of measurement used for the emissions.
\end{itemize}

These indicators are crucial for assessing a company's environmental impact and sustainability practices, and they are often reported in financial statements as part of the company's \ac{ESG} disclosures.
In essence, there are three main indicators analyzed, but since the \ac{GHG} Scope 2 emissions are divided into three categories, we consider them as separate indicators for this study.
It is also important to be able to extract the unit of measurement used for the emissions because different companies may report their emissions in different units, such as metric tons of \ce{CO2} equivalent, kilograms of \ce{CO2}, or other units.

\subsection{Extracted Data Schema}

The extracted data from the financial reports should follow a specific schema to ensure consistency and comparability across different strategies.
Since most of the strategies are \ac{LLM}-based, defining a schema adds complexity to the system, as hallucination could lead to correct data being extracted but in the wrong format.
For these reasons, we define a schema that is simple and straightforward, focusing on the key indicators of interest and their values for each year reported.
Here is an example of the \ac{JSON} schema for the extracted data:

\begin{minted}{json}
{
    "metrics": {
        "2022": {
            "scope_1": 88200000.0,
            "scope_2": {
                "location_based": null,
                "market_based": null,
                "undefined": 200000.0
            },
            "scope_3": 38800000.0
        },
        "2023": {
            "scope_1": 75100000.0,
            "scope_2": {
                "location_based": null,
                "market_based": null,
                "undefined": 200000.0
            },
            "scope_3": 36600000.0
        }
    },
    "extracted_pages": [1, 4, 10],
    "reported_unit": "million_metric_tonnes",
}
\end{minted}

This schema ensures that the extracted data can be processed by the system and represented in the same unit of measurement, ensuring that the comparison between the different strategies is fair and accurate.
The \texttt{extracted\_pages} field is used to store the page numbers from which the data was extracted, allowing for traceability and validation of the extracted information.

\subsection{Evaluation Criteria}
\label{sec:evaluation_criteria}

To thoroughly assess the performance of the proposed systems for extracting indicators from financial reports, a combination of quantitative and qualitative metrics is employed.
These metrics are designed to measure both the accuracy of the extracted data and the robustness of the extraction process against various types of errors.
Here, we delineate the key metrics and evaluation criteria used.

\subsubsection{Detection Rate}

Not only is it important to evaluate the accuracy of the extracted data, but it is also crucial to assess the system's ability to detect the presence of the indicators of interest in the document, as well as avoid detecting false positives.
The detection rate is calculated as the proportion of documents in which the system correctly identifies the indicators of interest.
In order to assess the detection rate, we utilize Precision, Recall, and Accuracy.

\begin{itemize}
    \item \textbf{Precision} assesses the proportion of data points extracted by the model that are correct and relevant.
    A high precision rate indicates fewer instances of fabricated metrics and irrelevant data extraction.

    \begin{equation}
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
    \end{equation}

    \item \textbf{Recall} measures the system's ability to retrieve all relevant data points from the document.
    High recall is essential to ensure no significant data is missed.

    \begin{equation}
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
    \end{equation}

    \item \textbf{Accuracy} provides an overall measure of the system's performance in detecting the indicators of interest.

    \begin{equation}
        \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{True Positives} + \text{True Negatives} + \text{False Positives} + \text{False Negatives}}
    \end{equation}
\end{itemize}

Ideally, the system should achieve high precision and recall rates to ensure that the extracted data is both accurate and comprehensive.
In practice, there is often a trade-off between precision and recall, and the system must be optimized to balance these metrics effectively.
In our case, we would prefer to minimize the number of false positives, as those would represent fabricated metrics --- perhaps due to \ac{LLM} hallucinations --- thus, we would slightly prioritize precision over recall.

\subsubsection{Residual Analysis}

While we want a system that has high recall and precision rates to guarantee that the presence of key information is being detected, it is also fundamental that this information is being extracted correctly (or at least as close as possible to the ground truth).
For this reason, we also conduct a residual analysis to evaluate how far the extracted values are from the ground truth values.
The residual is calculated by taking the absolute difference between the extracted value (\texttt{y}) and the ground truth value (\texttt{y\_hat}).

\begin{equation}
    \text{Residual} = |y - \hat{y}|
\end{equation}

The lower the residual, the closer the extracted value is to the ground truth value, indicating a more accurate extraction process.

\subsubsection{Perfect Match Rate}

A special case when the metric is correctly detected and also perfectly extracted is what the system ultimately aims to maximize, and in this study, we refer to it as the Perfect Match Rate.
This metric is calculated as the proportion of documents in which the system correctly identifies and extracts the indicators of interest without any errors.

\begin{equation}
    \text{Perfect Match Rate} = \frac{\text{Perfect Matches}}{\text{Total Documents}}
\end{equation}

\subsubsection{Error Types Analysis}

In addition to quantitative metrics, an error analysis is conducted to identify the types of errors made by the system during the extraction process.

\begin{itemize}
    \item \textbf{Missing Data}: Indicates data that should have been extracted but was not.
    This error impacts the \texttt{recall} metric.
    \item \textbf{Fabricated Metrics}: Metrics that are not present in the document but appear in the extracted output --- also referred to as extrinsic hallucination.
    This error impacts \texttt{precision}.
    \item \textbf{Incorrect Values}: Reflects errors in the value of the data extracted.
    These are numerical errors that impact the \texttt{residual} metric.
\end{itemize}

\subsection{Shared Processing Steps}

For all of the experiments, the system receives a \ac{PDF} as input, and common pre and post-processing steps are applied to the data to ensure that the system is able to extract the information from the reports.

\subsubsection{Pre-processing: Finding Pages of Interest}

A shared pre-processing step is to find the pages of interest --- those that might contain the information that we are looking for --- in the \ac{PDF} document.

\begin{minted}{python}
def extract_data_from_pdf(pdf_file: str):
  # Load the PDF file
  doc = fitz.open(pdf_file)

  for page in doc:
    # extract text from page
    text = page.get_text("text")

    # function that will look for matches
    # for the indicators of interest
    matches = search_for_keywords(text)

    # this page does not contain
    # any relevant information
    if matches is None:
        return

    # Then different strategies follow...
\end{minted}

This pre-processing step is used to iterate over the pages in the \ac{PDF} document and find the pages of interest so that the core engine is only applied to these pages.
This is an important step to reduce the processing time and avoid unnecessary processing of pages that do not contain relevant information.
However, it is also crucial to ensure that no false negatives are generated, as this could lead to missing pages of interest.

\subsubsection{Parsing LLM Outputs}

As of the time of writing, the text \ac{GPT}-4 model has a parameter flag to determine the output format of the model, while the vision model does not have this feature.
This means that for text, it is possible to specify \ac{JSON} as the output format.
However, for the vision model, the output is always a string, which leads to uncertainty about whether the output string can be parsed into a \ac{JSON} format by the system.
Additionally, the parameter flag does not guarantee that the output \ac{JSON} will contain the correct keys and meet the desired schema.
For these reasons, it is important to have a parsing step that validates the output of the \ac{LLM} models and ensures that the extracted data is in the expected format.

This is where the \textit{pydantic} and \textit{instructor} libraries come into play.
We can use \textit{pydantic} to define the model schema and validate fields and types, and \textit{instructor} will handle any imperfections in the model by retrying the generation process $n$ times and validating the output against the defined schema.

\subsubsection{Post-processing: Consolidating Information from Different Pages}

In an ideal scenario, all the information we are looking for would be contained on a single page of the \ac{PDF} document, allowing our system to find the page, extract the data, and return it.
However, this is not the case in practice, as different companies use varying designs to display their information, sometimes spreading the indicators across multiple pages or even repeating them in different sections of the report.
Since our system needs to resolve conflicts in scenarios where multiple pages contain the same information (potentially with different values due to local regulations, units, and so forth), a strategy for consolidating the information is required.
After testing various strategies like frequency bags, selecting the output with the most found values, and \ac{LLM}s, we decided to use the \ac{LLM}-based approach, leveraging the \ac{GPT}-4 model to consolidate the information from different pages.
Here is a demonstration of how this can be done:

\begin{minted}{python}
def consolidate_data(data: list[str]) -> str:
  """
  Consolidate data from different pages

  Args:
    data: List of JSON strings containing the data
          extracted from different pages

  Returns:
    consolidated_data: Unique JSON string containing
                       the consolidated data
  """
  # Send request with data and instructions to GPT-4
  consolidated_data = openai.chat.completions.create(
    model="gpt-4-turbo-preview",
      response_format={"type": "json"},
      messages=[
        {
          "role": "system",
          "content": """Consolidate the following data
                        into only one JSON string...""",
        },
        {"role": "user", "content": data},
      ]
    )
  return consolidated_data
\end{minted}

This is a simple example of how the \ac{GPT}-4 model can be used to consolidate information from different pages of the \ac{PDF} document.
This process can be further improved by adding more context and validation steps, such as checking for inconsistencies in the data before returning the consolidated information.

\subsection{Text-Only Approach with LLMs}

In this experiment, we use the \ac{GPT}-4 architecture, specifically \textit{gpt-4-0125-preview} released in December 2023, to extract information from the text contained in the financial reports without considering any images or visual data.
Evidently, this approach is limited to the information present in the text and does not account for any charts or important visual cues that the report might contain.
Therefore, it is expected that this system may fail in cases where the indicators are presented in a visual form.

\subsection{Image-Only Approach with LLMs}

In this experiment, we use the \ac{GPT}-4 Vision model to extract information from the images contained in the financial reports without considering any text data.
Since OpenAI's visual model is highly accurate in extracting information from images, this approach is expected to have an overall performance superior to the text-only approach because it can generalize the information from images.
However, it is also expected to have an increased propensity for hallucinations, as the model cannot validate the information extracted from the images with the text data.

For this experiment, after the preprocessing step, the system applies the following image-processing transforms to each of the pages of interest:

\begin{itemize}
\label{list:image_transforms}
    \item \ac{DPI}: The image is set to 300 \ac{DPI} to ensure that the model can extract the information with high quality.
    This setting has been shown to be one of the optimal settings for \ac{OCR} tasks \cite{ocr_preprocessing2007}.
    \item Grayscale: The image is converted to grayscale to reduce the amount of information that the model needs to process.
    \item Downscaling: The image is downscaled to fit OpenAI's \ac{GPT}-4 Vision criteria:
    \begin{quote}
        \textit{...images are first scaled to fit within a 2048 x 2048 square, maintaining their aspect ratio. Then, they are scaled such that the shortest side of the image is 768px long \cite{OpenAIVisionAPI}.}
    \end{quote}
    \item Compression: The image is compressed to a 90\% quality to significantly reduce the size of the image while maintaining sufficient quality for image analysis.
\end{itemize}

These transforms aim to reduce the amount of information that the model needs to process while maintaining the quality of the information extracted from the images.
This is important not only for model performance but also to keep the number of tokens processed lower, reducing the cost of the operation.

The system sends a request to OpenAI's \ac{GPT}-4 Vision model with the pre-processed image and instructions for the model to extract the key indicators from the image into a predefined JSON schema.
This is necessary and important because not only does the model need to be able to extract the information from the images, but it also needs to produce the information in a format that can be parsed by the system, otherwise, it might raise unexpected errors on the client side.

\subsection{Multimodal LLMs to Extract Information from Images and Text}

In this experiment, we use a multimodal approach to extract information from the financial reports, leveraging information from both textual and visual domains.
This approach combines the strengths of the text-only and image-only methods while mitigating their respective weaknesses by validating the information extracted from the images with the text data and vice versa.

The system follows the same preprocessing steps as the image-only approach before sending the image and instructions to the \ac{GPT}-4 Vision model as described in \ref{list:image_transforms}.
The difference in this experiment is that the system also extracts the text from the page and creates a set of all the numbers mentioned in the text, which will be used to validate whether the information provided by the model is consistent with the text data or if it is an extrinsic hallucination.

To mitigate intrinsic and extrinsic hallucinations, we implement a simple lookup method that searches for the closest value in the text data to the one extracted from the image, within a defined threshold limit.
Values within the threshold are considered valid and are readjusted, while those that fall outside the threshold are considered hallucinations and are discarded.

This system is expected to have the best performance among the three approaches, as it can validate the information extracted from the images with the text data and vice versa.
However, it is also expected to have the highest processing time due to the increased complexity of the system.

% TODO check below

To address the observed hallucination behavior --- particularly errors in the last digits of numeric values --- the system could benefit from additional validation checks.
For example, implementing a rule that flags unusually large discrepancies in the last digits could help mitigate such errors, ensuring greater accuracy in the final extracted values.

\clearpage
\section{Results}

In this section, we explore the results of the experiments conducted to extract information from financial reports using different strategies, assessing the metrics defined in Section \ref{sec:evaluation_criteria}.
The results are presented in tabular format, showing the performance of each system in terms of detection rate, residual analysis, perfect match rate, and error type analysis.
Charts are also used to ease the interpretability of the results.

\subsection{Detection Rate}

The detection rate is a crucial metric for evaluating the system's ability to identify the indicators of interest in the financial reports while also accounting for the presence of false positives, which could lead to fabricated indicators.

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{
    \ra{1.3}
    \begin{tabular}{@{}llrrrrr@{}}\toprule
        \textbf{Approach} & \textbf{Indicator} & \textbf{Recall} & \textbf{Precision} & \textbf{Accuracy} \\ \midrule
        \textbf{Text Only} & Scope 1 Emissions & 0.97 & 0.90 & 0.88 \\
        & Scope 2 Emissions & 0.05 & 0.33 & \textbf{0.74} \\
        & Scope 2 Emissions Location & 0.91 & 0.62 & 0.63 \\
        & Scope 2 Emissions Market & \textbf{1.00} & 0.48 & 0.81 \\
        & Scope 3 Emissions & 0.91 & 0.78 & 0.85 \\
        \midrule
        \textbf{Vision Only} & Scope 1 Emissions & 0.90 & 0.87 & 0.81 \\
        & Scope 2 Emissions & \textbf{0.08} & 0.33 & 0.68 \\
        & Scope 2 Emissions Location & 0.82 & \textbf{0.64} & \textbf{0.64} \\
        & Scope 2 Emissions Market & 0.85 & 0.38 & 0.77 \\
        & Scope 3 Emissions & 0.77 & 0.83 & 0.83 \\
        \midrule
        \textbf{Multimodal} & Scope 1 Emissions & \textbf{0.98} & \textbf{0.95} & \textbf{0.94} \\
        & Scope 2 Emissions & 0.05 & \textbf{1.00} & 0.69 \\
        & Scope 2 Emissions Location & \textbf{0.97} & 0.60 & 0.63 \\
        & Scope 2 Emissions Market & 0.92 & \textbf{0.67} & \textbf{0.89} \\
        & Scope 3 Emissions & \textbf{0.93} & \textbf{0.90} & \textbf{0.92} \\
        \bottomrule
    \end{tabular}}
    \caption{Metrics for different approaches.}
    \label{tab:approach_metrics}
\end{table}

In terms of recall, the multimodal approach demonstrates the highest recall rates for most indicators, particularly \textit{Scope 1 emissions} and \textit{Scope 3 emissions}, where it achieves 0.98 and 0.93, respectively.
The text-only approach, while generally reliable, struggles with indicators that involve visual elements, such as \textit{Scope 2 emissions}, where its recall drops significantly to 0.05.
The image-only approach performs similarly to the text-only approach in some areas, but it is generally less consistent, with recall rates varying widely across different indicators.

Precision is another area where the multimodal approach excels.
For most indicators, including \textit{Scope 1 emissions} and \textit{Scope 3 emissions}, the multimodal strategy achieves the highest precision, reaching 0.95 and 0.90, respectively.
This indicates that the multimodal approach is not only effective in identifying relevant data but also minimizes the extraction of incorrect or irrelevant information.
Notably, the precision for \textit{Scope 2 emissions} under the multimodal approach reaches 1.00, suggesting that when the system does detect this indicator, it is highly accurate.

Accuracy, which provides an overall measure of performance, is consistently highest for the multimodal approach across most indicators.
For \textit{Scope 1 emissions} and \textit{Scope 3 emissions}, the accuracy reaches 0.94 and 0.92, respectively, reinforcing the superiority of the multimodal method in extracting accurate data.
The text-only approach, while accurate for certain indicators, suffers in cases where visual data is critical, as seen in its lower accuracy for \textit{Scope 2 emissions}.

The findings suggest that in a real-world scenario, where the goal is to either expedite the extraction of information from financial reports or automate the process entirely, the multimodal approach presents the most promising strategy.
Its ability to integrate and cross-validate text and visual data makes it particularly well-suited for handling the diverse and complex nature of financial documents.

\subsection{Perfect Match Rate}

\subsubsection{Text-only}

This experiment focused on extracting the information based only on text data from the document pages.
Since some documents present the information in a structured way, the text-only approach was able to extract the information with a high precision and recall rate, as shown in Table \ref{tab:text_only_metrics}.
However, as previously discussed in Section \ref{sec:dataset}, there are documents that present visually rich information, such as charts and tables, which might be challenging for the text-only approach to extract the information correctly.
These documents will often produce a mismatched prediction, confusing different metrics (e.g., assigning the value of Scope 1 emissions to Scope 3 emissions or vice versa).
Because of the nature of the indicators being analyzed, these metrics can often be in different orders of magnitude, which can lead to high residuals when the system makes a mistake in the extraction process.
Therefore, we see that the residual values are high for all the indicators, indicating that the system is not able to extract the information correctly in some cases.
However, the perfect match rate is mostly above 70\% for all the indicators, indicating that the system was able to correctly predict the indicators in a high proportion of the documents.

\begin{table}[H]\centering
    \resizebox{\textwidth}{!}{
    \ra{1.3}
    \begin{tabular}{@{}llrrrrrrr@{}}\toprule
        \textbf{Indicator} & \textbf{Metric} & \textbf{Mean} & \textbf{Std} & \textbf{Min} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Max} \\ \midrule
        \textbf{Scope 1 emissions} & residual & 621589.32 & 2617185.82 & 0.0 & 0.0 & 0.0 & 300.5 & 16657326.0 \\
        & perfect match & 70\% & & & & & & \\
        \textbf{Scope 2 emissions} & residual & 42701.47 & 227479.61 & 0.0 & 0.0 & 0.0 & 72.45 & 1534000.0 \\
        & perfect match & 73\% & & & & & & \\
        \textbf{Scope 2 emissions location} & residual & 1093202.00 & 6180676.24 & 0.0 & 0.0 & 0.0 & 12355.0 & 49315635.0 \\
        & perfect match & 53\% & & & & & & \\
        \textbf{Scope 2 emissions market} & residual & 358920.45 & 2260835.10 & 0.0 & 0.0 & 0.0 & 0.0 & 16997182.0 \\
        & perfect match & 79\% & & & & & & \\
        \textbf{Scope 3 emissions} & residual & 785785.79 & 5287354.29 & 0.0 & 0.0 & 0.0 & 2.07 & 46199953.8 \\
        & perfect match & 74\% & & & & & & \\
        \bottomrule
    \end{tabular}}
    \caption{Residual and perfect match metrics for the text-only approach.}
    \label{tab:text_only_metrics}
\end{table}

\subsubsection{Image-only}

The image-only approach focuses on extracting information from the \ac{PDF} document pages, treating them as images.
This approach is expected to perform better than the text-only approach for documents that contain visually rich information, such as charts and tables, as the model is able to interpret the visual nuances of the document.
However, it is also expected to have a higher rate of hallucinations, as the model is not able to validate the information extracted from the images with the text data.

\begin{table}[H]\centering
    \resizebox{\textwidth}{!}{
    \ra{1.3}
    \begin{tabular}{@{}llrrrrrrr@{}}\toprule
        \textbf{Indicator} & \textbf{Metric} & \textbf{Mean} & \textbf{Std} & \textbf{Min} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Max} \\ \midrule
        \textbf{Scope 1 emissions} & residual & 1537568.10 & 12489496.27 & 0.0 & 0.0 & 0.0 & 2435.38 & 116455366.0 \\
        & perfect match & 51\% & & & & & & \\
        \textbf{Scope 2 emissions} & residual & 28025.10 & 167960.05 & 0.0 & 0.0 & 0.0 & 601.32 & 1534000.0 \\
        & perfect match & 67\% & & & & & & \\
        \textbf{Scope 2 emissions location} & residual & 1608246.63 & 12893554.83 & 0.0 & 0.0 & 28.18 & 5243.06 & 119880861.0 \\
        & perfect match & 45\% & & & & & & \\
        \textbf{Scope 2 emissions market} & residual & 127889.60 & 1142411.28 & 0.0 & 0.0 & 0.0 & 5.44 & 10718989.28 \\
        & perfect match & 72\% & & & & & & \\
        \textbf{Scope 3 emissions} & residual & 859217.55 & 5387118.98 & 0.0 & 0.0 & 0.0 & 215.17 & 46199953.8 \\
        & perfect match & 66\% & & & & & & \\
        \bottomrule
    \end{tabular}}
    \caption{Residual and perfect match metrics for the image-only approach.}
    \label{tab:image_only_metrics}
\end{table}

It is notable that the visual model underperforms the text-only approach in almost all the evaluated metrics.
This could be due to the fact that rich-visual information is not present in most documents and therefore this model would outperform the text-only approach in a subset of the documents present in the dataset while underperforming in the majority of the documents.

\subsubsection{Multimodal}

The multimodal approach combines the information extracted from the text and images to predict the indicators of interest from the financial reports.
This approach is expected to have the best performance among the three approaches, as it is able to use the information from both the text and images to validate the information extracted from the images with the text data and vice versa.

\begin{table}[H]\centering
    \resizebox{\textwidth}{!}{
    \ra{1.3}
    \begin{tabular}{@{}llrrrrrrr@{}}\toprule
        \textbf{Indicator} & \textbf{Metric} & \textbf{Mean} & \textbf{Std} & \textbf{Min} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Max} \\ \midrule
        \textbf{Scope 1 emissions} & residual & 751.58 & 3601.29 & 0.0 & 0.0 & 0.0 & 0.0 & 25030.0 \\
        & perfect match & 89\% & & & & & & \\
        \textbf{Scope 2 emissions} & residual & 8431.00 & 32265.61 & 0.0 & 0.0 & 0.0 & 82.91 & 239308.0 \\
        & perfect match & 69\% & & & & & & \\
        \textbf{Scope 2 emissions location} & residual & 8680.55 & 30929.10 & 0.0 & 0.0 & 0.0 & 4834.0 & 239308.0 \\
        & perfect match & 57\% & & & & & & \\
        \textbf{Scope 2 emissions market} & residual & 584.96 & 2194.31 & 0.0 & 0.0 & 0.0 & 0.0 & 12560.0 \\
        & perfect match & 88\% & & & & & & \\
        \textbf{Scope 3 emissions} & residual & 227871.45 & 1168999.93 & 0.0 & 0.0 & 0.0 & 0.0 & 8254000.0 \\
        & perfect match & 85\% & & & & & & \\
        \bottomrule
    \end{tabular}}
    \caption{Residual and perfect match metrics for the multimodal approach.}
    \label{tab:multimodal_metrics}
\end{table}

The results confirm that the multimodal approach outperforms the text-only and image-only approaches in almost all the evaluated metrics by a significant margin.
Additionally, most indicators have 75\% of the documents with a residual of 0, indicating that the system was able to extract the information correctly in most of the cases while also holding a perfect match rate above 85\% for metrics \textit{Scope 1 emissions}, \textit{Scope 2 emissions market} and \textit{Scope 3 emissions}.

\subsubsection{How the Approaches Compare}

When analyzing the perfect match rates for each indicator in each experiment strategy, it is clear that the multimodal approach outperforms the text-only and image-only approaches, which perform similarly, with a slight advantage for the text-only approach.
Considering the scopes that do not have subdivisions, it is observable that there is a considerable improvement in the multimodal approach against the text approach, reaching a 19\% improvement in the perfect match rate for the \textit{Scope 1 emissions} indicator and an 11\% improvement for the \textit{Scope 3 emissions} indicator.
In a context where this system is being assessed for use in a real-world scenario --- whether to expedite the process of extracting information from financial reports or to automate the process --- the multimodal approach appears to be the right strategy to fine-tune and build upon.

% display image
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/perfect_match_comparison.png}
    \caption{Results of the experiments for the different approaches.}
    \label{fig:perfect_match_comparison}
\end{figure}

\subsection{How Significant Are the System's Errors?}

While the perfect match rate provides a meaningful and straightforward way to compare the experiments, it is crucial to understand the severity of the errors when a perfect match is not achieved.
For instance, consider a scenario where a company reports its \textit{Scope 1 emissions} as 300,000 MT \ce{CO2}, but the system extracts this value as 300,020 MT \ce{CO2}.
Although this is not a perfect match, the error is so small that it could be considered negligible in some cases.
The sensitivity to such errors may vary depending on the specific indicators and the business context in which the extracted data is used.

The charts below represent the residuals for each indicator in each experimental strategy, arranged from the highest to the lowest residual value.
Ideally, the best-performing system would have a curve closer to the origin, indicating smaller errors, while the worst-performing system would have a curve closer to the top-right corner of the chart.

% display two images side by side
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/scope1_residuals.png}
        \caption{Scope 1 emissions residual}
        \label{fig:scope_1_residual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/scope3_residuals.png}
        \caption{Scope 3 emissions residual}
        \label{fig:scope_3_residual}
    \end{subfigure}
    \caption{Residual analysis for Scope 1 and Scope 3 emissions.}
    \label{fig:scope_1_3_residual_analysis}
\end{figure}

% display three images side by side
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{images/scope2_location_residuals.png}
        \caption{Location-based}
        \label{fig:scope_2_location_residual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{images/scope2_market_residuals.png}
        \caption{Market-based}
        \label{fig:scope_2_market_residual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{images/scope2_residuals.png}
        \caption{Undefined}
        \label{fig:scope_2_residual}
    \end{subfigure}
    \caption{Residual analysis for Scope 2 emissions.}
    \label{fig:scope_2_residual_analysis}
\end{figure}

A closer examination of the residuals reveals that the vision model produces samples with higher residuals than the text model, despite the text model generally having a higher perfect match rate.
This observation is interesting as it indicates that these models make different types of errors.
If the primary goal is to maximize the perfect match rate, the text model outperforms the vision model.
However, if minimizing residuals is the main objective, the vision model demonstrates better performance.

Overall, the multimodal approach offers the best performance in terms of residuals.
In the case of the \textit{Scope 2 Residuals} indicator chart, the vision model seems to produce fewer samples with high residuals, but this is due to the fact that the vision model predicted fewer samples for this indicator.

For the \textit{Scope 3 Residuals} indicator chart, the vision model performs very closely to the multimodal model, highlighting a weaker performance by the text model for this particular indicator.
This is likely due to the loss of information that the text model experiences compared to the vision model.
Since the \textit{Scope 3 emissions} indicator often has the largest values --- due to encompassing all indirect emissions --- the residuals tend to be higher for this indicator if there is confusion with another indicator.

\subsection{Considerations}

All experiments show poorer performance for the sub-indicators under \textit{Scope 2 emissions} compared to the other indicators.
This occurs because \textit{Scope 2 emissions} are divided into three categories: location-based, market-based, and undefined (when the document does not clearly specify the measurement standard).
The system struggles to differentiate between these categories, leading to higher residuals and lower perfect match rates for these indicators.

In many cases, what is considered a mistake may actually be a correct extraction, but the labeling might be unclear.
For instance, the model may infer that Company A is reporting $300,349$ MT \ce{CO2} as location-based emissions when the document does not specify the nature of the emissions, and the label refers to it as undefined.
This type of error falls into a gray area, where the labeled dataset would need to be revised to ensure that the nature of the emissions is perfectly labeled.
However, due to time constraints, this will not be possible in this study.

The metrics and labels reported for Scopes 1 and 3 should not suffer from this uncertainty, making the results more reliable for these indicators and more suitable for comparison between the different approaches.

\clearpage

\section{Conclusions}
\label{sec:summary}

This thesis explored various approaches for extracting \ac{GHG} emissions from financial reports using \acp{LLM} as the core engine behind the predictions.
The primary objective was to evaluate the effectiveness of text-only, image-only, and multimodal strategies in accurately identifying and extracting key environmental metrics from complex financial documents.
Through a series of experiments, each method was assessed using a robust set of evaluation criteria, including detection rate, residual analysis, and perfect match rate.
The results provide critical insights into the strengths and limitations of each approach, allowing for a better understanding of the trade-offs posed by each method.

The text-only approach demonstrated commendable performance, particularly in handling structured text data --- such as tables and indicators disclosed in plain text --- achieving reasonably high precision and recall across most indicators.
However, its reliance solely on textual information rendered it less effective in scenarios where critical data was embedded in visually rich elements, such as charts, illustrations, and unconventional table formats.
This limitation was evident in the higher residuals and occasional mismatches in indicator predictions, underscoring the challenges of relying on text-based extraction methods for visually complex documents.

The image-only approach, while capable of interpreting visually rich information, underperformed relative to the text-only method.
Although it excelled in extracting data from images, its propensity for hallucinations and the absence of textual validation led to lower precision and increased errors in the extracted metrics.
The inherent weaknesses in the image-only approach highlight the difficulties of extracting accurate data when solely dependent on visual cues, particularly in financial documents where both text and images convey essential information.
This method also performed poorly on the given dataset because, as observed during the annotation process, when companies provide visual-rich features to disclose \ac{GHG} emissions, the same information is often found on another page of the report in a more traditional table format.
This gave the text-only approach an advantage, as there would usually be a text-friendly table, reducing confusion in deciding which page contains the correct information on the key indicators.

The multimodal approach, which integrates both text and image data, proved to be the most effective strategy.
By leveraging the complementary strengths of text and image analysis, this method achieved the highest overall accuracy, precision, and recall for detections, as well as higher perfect match rates exceeding 85\% for some key indicators.
The multimodal approach also demonstrated superior performance in residual analysis, indicating its capability to extract data with minimal errors, keeping the system's errors more manageable and less harmful.
This method's ability to cross-validate information between text and images effectively mitigated the risk of hallucinations and significantly reduced extraction errors, making it the most robust solution for \ac{ESG} data extraction in financial reports across the studied methods.

Despite the overall success of the multimodal approach, the study also identified areas for improvement.
The challenges associated with differentiating sub-indicators under Scope 2 emissions, due to classification ambiguities, suggest that further refinement of the dataset labeling and perhaps a more forgiving validation for such indicators could enhance the accuracy of the extraction process while maintaining the interpretability of the results.

In conclusion, this thesis contributes to the field of information retrieval from financial documents by demonstrating the power of \acp{LLM} in automating the extraction of \ac{ESG} indicators from such a diverse real-world dataset.
The findings emphasize the importance of a multimodal approach for achieving high accuracy and reliability in complex document analysis tasks.
Future research could explore the integration of more advanced model preprocessing techniques to increase the focus on key elements of the page, such as cropping tables or infographics containing the key indicators to avoid showing the model irrelevant information. Moreover, expanding the scope of indicators and refining the evaluation metrics would provide a more comprehensive understanding of \acp{LLM}'s capabilities and limitations in this domain.
Overall, the insights gained from this study lay the groundwork for further advancements in the automated analysis of financial reports, with significant implications for businesses, regulatory compliance, and sustainability initiatives.

\clearpage
%% Bibliography/ list of references
%%
\thesisbibliography

%reference bib file
\bibliographystyle{IEEEtranDOI}
\bibliography{references}

%% Appendices
%% If you don't have appendices, your thesis ends here. Remove \clearpage,
%% \thesisappendix and the following text below. The last command of this file
%% is \end{document}.
% \clearpage

% \thesisappendix

% \section{Contents of an appendix}
% \label{app:contents}

\end{document}
