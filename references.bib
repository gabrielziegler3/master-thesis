@article{Xu2024,
g  abstract = {Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.},
   author = {Ziwei Xu and Sanjay Jain and Mohan Kankanhalli},
   month = {1},
   title = {Hallucination is Inevitable: An Innate Limitation of Large Language Models},
   url = {http://arxiv.org/abs/2401.11817},
   year = {2024},
}
@article{Wang2023,
   abstract = {Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.},
   author = {Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},
   month = {12},
   title = {DocLLM: A layout-aware generative language model for multimodal document understanding},
   url = {http://arxiv.org/abs/2401.00908},
   year = {2023},
}
@article{OpenAI2023GPT4,
   abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
   author = {OpenAI},
   month = {3},
   title = {GPT-4 Technical Report},
   url = {http://arxiv.org/abs/2303.08774},
   year = {2023},
}
@article{Tang2022,
   abstract = {We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark.},
   author = {Zineng Tang and Ziyi Yang and Guoxin Wang and Yuwei Fang and Yang Liu and Chenguang Zhu and Michael Zeng and Cha Zhang and Mohit Bansal},
   month = {12},
   title = {Unifying Vision, Text, and Layout for Universal Document Processing},
   url = {http://arxiv.org/abs/2212.02623},
   year = {2022},
}
@inproceedings{Huang2022,
   abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3.},
   author = {Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
   doi = {10.1145/3503161.3548112},
   isbn = {9781450392037},
   journal = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
   keywords = {LayoutLM,document AI,multimodal pre-training,vision-and-language},
   month = {10},
   pages = {4083-4091},
   publisher = {Association for Computing Machinery, Inc},
   title = {LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
   year = {2022},
}
@article{Wei2022,
   abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
   author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
   month = {1},
   title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
   url = {http://arxiv.org/abs/2201.11903},
   year = {2022},
}
@article{Cui2021,
   abstract = {Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.},
   author = {Lei Cui and Yiheng Xu and Tengchao Lv and Furu Wei},
   month = {11},
   title = {Document AI: Benchmarks, Models and Applications},
   url = {http://arxiv.org/abs/2111.08609},
   year = {2021},
}
@article{Wang2021,
   abstract = {Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. We will release the dataset and model at \url\{https://aka.ms/layoutreader\}.},
   author = {Zilong Wang and Yiheng Xu and Lei Cui and Jingbo Shang and Furu Wei},
   month = {8},
   title = {LayoutReader: Pre-training of Text and Layout for Reading Order Detection},
   url = {http://arxiv.org/abs/2108.11591},
   year = {2021},
}
@inproceedings{Xu2020,
   abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.},
   author = {Yiheng Xu and Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou},
   doi = {10.1145/3394486.3403172},
   isbn = {9781450379984},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {LayoutLM,document image understanding,pre-trained models},
   month = {8},
   pages = {1192-1200},
   publisher = {Association for Computing Machinery},
   title = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},
   year = {2020},
}
@inproceedings{2023GPT4VisionSC,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263218031}
}
@inproceedings{lewis2020retrievalaugmented,
   author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
   title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
   year = {2020},
   isbn = {9781713829546},
   publisher = {Curran Associates Inc.},
   address = {Red Hook, NY, USA},
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
   articleno = {793},
   numpages = {16},
   location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
   series = {NIPS '20}
}
@misc{xu2024hallucination,
      title={Hallucination is Inevitable: An Innate Limitation of Large Language Models},
      author={Ziwei Xu and Sanjay Jain and Mohan Kankanhalli},
      year={2024},
      eprint={2401.11817},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{huang2023surveyhallucination,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{Brown2020,
      title={Language Models are Few-Shot Learners},
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@software{Chase_LangChain_2022,
   author = {Chase, Harrison},
   month = oct,
   title = {{LangChain}},
   url = {https://github.com/langchain-ai/langchain},
   year = {2022}
}
@software{Instructor2023,
   author = {Liu, Jason},
   month = jun,
   title = {{Instructor: Structured LLM Outputs}},
   url = {https://github.com/jxnl/instructor},
   year = {2023}
}
@software{pydantic,
  title = {Pydantic},
  author = {Samuel Colvin and Eric Jolibois and Hasan Ramezani and Adrian Garcia Badaracco and Terrence Dorsey and David Montague and Serge Matveenko and Marcelo Trylesinski and Sydney Runkle and David Hewitt and Alex Hall},
  year = {2024},
  month = {3},
  version = {v2.6.4},
  date = {2024-03-12},
  url = {https://docs.pydantic.dev/latest/},
  abstract = {Pydantic is the most widely used data validation library for Python. Fast and extensible, Pydantic plays nicely with your linters/IDE/brain. Define how data should be in pure, canonical Python 3.8+; validate it with Pydantic.},
  keywords = {python, validation, parsing, json-schema, hints, typing},
  license = {MIT},
  repository-code = {https://github.com/pydantic/pydantic}
}
@article{survey_hallucination_natural_language_generation,
   author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
   title = {Survey of Hallucination in Natural Language Generation},
   year = {2023},
   issue_date = {December 2023},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   volume = {55},
   number = {12},
   issn = {0360-0300},
   url = {https://doi.org/10.1145/3571730},
   doi = {10.1145/3571730},
   abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
   journal = {ACM Comput. Surv.},
   month = {mar},
   articleno = {248},
   numpages = {38},
   keywords = {Hallucination, intrinsic hallucination, extrinsic hallucination, faithfulness in NLG, factuality in NLG, consistency in NLG}
}
@article{castelvecchi2016can,
  title={Can we open the black box of AI?},
  author={Castelvecchi, Davide},
  journal={Nature News},
  volume={538},
  number={7623},
  pages={20},
  year={2016}
}
@article{2018_survey_explaining_black_box,
   author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
   title = {A Survey of Methods for Explaining Black Box Models},
   year = {2018},
   issue_date = {September 2019},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   volume = {51},
   number = {5},
   issn = {0360-0300},
   url = {https://doi.org/10.1145/3236009},
   doi = {10.1145/3236009},
   abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
   journal = {ACM Comput. Surv.},
   month = {aug},
   articleno = {93},
   numpages = {42},
   keywords = {Open the black box, explanations, interpretability, transparent models}
}
@INPROCEEDINGS{ocr_preprocessing2007,
  author={Bieniecki, Wojciech and Grabowski, Szymon and Rozenberg, Wojciech},
  booktitle={2007 International Conference on Perspective Technologies and Methods in MEMS Design},
  title={Image Preprocessing for Improving OCR Accuracy},
  year={2007},
  volume={},
  number={},
  pages={75-80},
  keywords={Optical character recognition software;Digital cameras;Text recognition;Image recognition;Image resolution;Solid modeling;Gaussian noise;Character recognition;Application software;Filters;Image preprocessing;OCR;digital cameras},
  doi={10.1109/MEMSTECH.2007.4283429}
}
@misc{OpenAIVisionAPI,
  title = {OpenAI Vision API Documentation},
  howpublished = {\url{https://platform.openai.com/docs/guides/vision}},
  note = {Accessed: 2024-04-11},
  organization = {OpenAI},
  year = {2024}
}
@misc{adobe2023pdftimeline,
  title = {PDF Timeline},
  author = {{Adobe Systems Incorporated}},
  year = {2023},
  howpublished = {\url{https://www.adobe.com/acrobat/resources/pdf-timeline.html}},
  note = {Accessed: 2024-04-20}
}
@inproceedings{Vaswani2017,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
    title = {Attention is all you need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}
@book{finkel2015cuneiform,
  title={Cuneiform},
  author={Finkel, I.L. and Taylor, J.},
  isbn={9781606064474},
  lccn={2014952411},
  series={Ancient scripts},
  url={https://books.google.com.br/books?id=cf7NrQEACAAJ},
  year={2015},
  publisher={J. Paul Getty Museum}
}
@inproceedings{table_extraction_conditional_pinto_2003,
author = {Pinto, David and McCallum, Andrew and Wei, Xing and Croft, W. Bruce},
title = {Table extraction using conditional random fields},
year = {2003},
isbn = {1581136463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/860435.860479},
doi = {10.1145/860435.860479},
abstract = {The ability to find tables and extract information from them is a necessary component of data mining, question answering, and other information retrieval tasks. Documents often contain tables in order to communicate densely packed, multi-dimensional information. Tables do this by employing layout patterns to efficiently indicate fields and records in two-dimensional form.Their rich combination of formatting and content present difficulties for traditional language modeling techniques, however. This paper presents the use of conditional random fields (CRFs) for table extraction, and compares them with hidden Markov models (HMMs). Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better. We show experimental results on plain-text government statistical reports in which tables are located with 92\% F1, and their constituent lines are classified into 12 table-related categories with 94\% accuracy. We also discuss future work on undirected graphical models for segmenting columns, finding cells, and classifying them as data cells or label cells.},
booktitle = {Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval},
pages = {235–242},
numpages = {8},
keywords = {tables, question answering, metadata, information extraction, hidden Markov models, conditional random fields},
location = {Toronto, Canada},
series = {SIGIR '03}
}
@article{PENG2006963,
title = {Information extraction from research papers using conditional random fields},
journal = {Information Processing \& Management},
volume = {42},
number = {4},
pages = {963-979},
year = {2006},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2005.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306457305001172},
author = {Fuchun Peng and Andrew McCallum},
keywords = {Information extraction, Constraint information extraction, Conditional random fields, Regularization},
abstract = {With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and globle layout features. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction; i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6–14%.}
}
@inproceedings{Fang2004,
author = {Fang, Hui and Tao, Tao and Zhai, ChengXiang},
title = {A formal study of information retrieval heuristics},
year = {2004},
isbn = {1581138814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008992.1009004},
doi = {10.1145/1008992.1009004},
abstract = {Empirical studies of information retrieval methods show that good retrieval performance is closely related to the use of various retrieval heuristics, such as TF-IDF weighting. One basic research question is thus what exactly are these "necessary" heuristics that seem to cause good retrieval performance. In this paper, we present a formal study of retrieval heuristics. We formally define a set of basic desirable constraints that any reasonable retrieval function should satisfy, and check these constraints on a variety of representative retrieval functions. We find that none of these retrieval functions satisfies all the constraints unconditionally. Empirical results show that when a constraint is not satisfied, it often indicates non-optimality of the method, and when a constraint is satisfied only for a certain range of parameter values, its performance tends to be poor when the parameter is out of the range. In general, we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraints. Thus the proposed constraints provide a good explanation of many empirical observations and make it possible to evaluate any existing or new retrieval formula analytically.},
booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {49–56},
numpages = {8},
keywords = {TF-IDF weighting, constraints, formal models, retrieval heuristics},
location = {Sheffield, United Kingdom},
series = {SIGIR '04}
}
@article{Bush1945As,
  added-at = {2011-07-12T19:07:55.000+0200},
  address = {New York, NY, USA},
  author = {Bush, Vannevar},
  biburl = {https://www.bibsonomy.org/bibtex/2b105f7d0996d9acb4f15677cc6b5a2a0/mstrohm},
  citeulike-article-id = {552920},
  citeulike-linkout-0 = {http://www.theatlantic.com/doc/194507/bush},
  citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=227186},
  citeulike-linkout-2 = {http://dx.doi.org/10.1145/227181.227186},
  doi = {10.1145/227181.227186},
  interhash = {eb70d415d74ed399cd08898c755d4683},
  intrahash = {b105f7d0996d9acb4f15677cc6b5a2a0},
  issn = {1072-5520},
  journal = {Atlantic Monthly},
  keywords = {},
  month = {March},
  number = 1,
  pages = {641--649},
  posted-at = {2009-01-04 14:49:31},
  priority = {0},
  publisher = {ACM},
  timestamp = {2011-09-22T11:56:28.000+0200},
  title = {{As We May Think}},
  url = {http://www.theatlantic.com/doc/194507/bush},
  volume = 176,
  year = 1945
}
@article{Salton1975,
author = {Salton, G. and Wong, A. and Yang, C. S.},
title = {A vector space model for automatic indexing},
year = {1975},
issue_date = {Nov. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/361219.361220},
doi = {10.1145/361219.361220},
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
journal = {Commun. ACM},
month = {nov},
pages = {613–620},
numpages = {8},
keywords = {document space, content analysis, automatic information retrieval, automatic indexing}
}
@article{Wong1987,
author = {Wong, S. K.M. and Ziarko, W. and Raghavan, V. V. and Wong, P. C.N.},
title = {On modeling of information retrieval concepts in vector spaces},
year = {1987},
issue_date = {June 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/22952.22957},
doi = {10.1145/22952.22957},
abstract = {The Vector Space Model (VSM) has been adopted in information retrieval as a means of coping with inexact representation of documents and queries, and the resulting difficulties in determining the relevance of a document relative to a given query. The major problem in employing this approach is that the explicit representation of term vectors is not known a priori. Consequently, earlier researchers made the assumption that the vectors corresponding to terms are pairwise orthogonal. Such an assumption is clearly unrealistic. Although attempts have been made to compensate for this assumption by some separate, corrective steps, such methods are ad hoc and, in most cases, formally inconsistent.In this paper, a generalization of the VSM, called the GVSM, is advanced. The developments provide a solution not only for the computation of a measure of similarity (correlation) between terms, but also for the incorporation of these similarities into the retrieval process.The major strength of the GVSM derives from the fact that it is theoretically sound and elegant. Furthermore, experimental evaluation of the model on several test collections indicates that the performance is better than that of the VSM. Experiments have been performed on some variations of the GVSM, and all these results have also been compared to those of the VSM, based on inverse document frequency weighting. These results and some ideas for the efficient implementation of the GVSM are discussed.},
journal = {ACM Trans. Database Syst.},
month = {jun},
pages = {299–321},
numpages = {23}
}
@INPROCEEDINGS{Yang2017,
  author={Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks},
  year={2017},
  volume={},
  number={},
  pages={4342-4351},
  keywords={Semantics;Visualization;Decoding;Training;Image reconstruction;Image segmentation},
  doi={10.1109/CVPR.2017.462}
}
@INPROCEEDINGS{Schreiber2017,
  author={Schreiber, Sebastian and Agne, Stefan and Wolf, Ivo and Dengel, Andreas and Ahmed, Sheraz},
  booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
  title={DeepDeSRT: Deep Learning for Detection and Structure Recognition of Tables in Document Images},
  year={2017},
  volume={01},
  number={},
  pages={1162-1167},
  keywords={Portable document format;Image recognition;Machine learning;Training;Metadata;Task analysis;Feature extraction},
  doi={10.1109/ICDAR.2017.192}
}
@article{singhal2001,
  title={Modern information retrieval: A brief overview},
  author={Singhal, Amit and others},
  journal={IEEE Data Eng. Bull.},
  volume={24},
  number={4},
  pages={35--43},
  year={2001}
}
@article{Maron1960,
   author = {Maron, M. E. and Kuhns, J. L.},
   title = {On Relevance, Probabilistic Indexing and Information Retrieval},
   year = {1960},
   issue_date = {July 1960},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   volume = {7},
   number = {3},
   issn = {0004-5411},
   url = {https://doi.org/10.1145/321033.321035},
   doi = {10.1145/321033.321035},
   abstract = {This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called “Probabilistic Indexing,” allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the “relevance number”) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance.The paper goes on to show that whereas in a conventional library system the cross-referencing (“see” and “see also”) is based solely on the “semantical closeness” between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected.Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.},
   journal = {J. ACM},
   month = {jul},
   pages = {216–244},
   numpages = {29}
}
@article{Nishant2020,
  author       = {Nishant Subramani and
                  Alexandre Matton and
                  Malcolm Greaves and
                  Adrian Lam},
  title        = {A Survey of Deep Learning Approaches for {OCR} and Document Understanding},
  journal      = {CoRR},
  volume       = {abs/2011.13534},
  year         = {2020},
  url          = {https://arxiv.org/abs/2011.13534},
  eprinttype    = {arXiv},
  eprint       = {2011.13534},
  timestamp    = {Tue, 01 Dec 2020 14:59:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2011-13534.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Wang2021,
    title = "{L}ayout{R}eader: Pre-training of Text and Layout for Reading Order Detection",
    author = "Wang, Zilong  and
      Xu, Yiheng  and
      Cui, Lei  and
      Shang, Jingbo  and
      Wei, Furu",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.389",
    doi = "10.18653/v1/2021.emnlp-main.389",
    pages = "4735--4744",
    abstract = "Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. The dataset and models are publicly available at \url{https://aka.ms/layoutreader}.",
}
@inproceedings{Li2022,
author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
title = {DiT: Self-supervised Pre-training for Document Image Transformer},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547911},
doi = {10.1145/3503161.3547911},
abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 - 92.69), document layout analysis (91.0 - 94.9), table detection (94.23 - 96.55) and text detection for OCR (93.07 - 94.29). The code and pre-trained models are publicly available at https://aka.ms/msdit.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3530–3539},
numpages = {10},
keywords = {OCR, document image classification, document image transformer, document layout analysis, self-supervised pre-training, table detection, text detection},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}
@inproceedings{Li2023,
author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
title = {TrOCR: transformer-based optical character recognition with pre-trained models},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i11.26538},
doi = {10.1609/aaai.v37i11.26538},
abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and word piece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1469},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}
@inproceedings{Xu2020,
author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
title = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403172},
doi = {10.1145/3394486.3403172},
abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1192–1200},
numpages = {9},
keywords = {LayoutLM, document image understanding, pre-trained models},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}
@inproceedings{Rosenblatt1958,
  author = {Rosenblatt, Frank},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  year = {1958},
  isbn = {9780262181114},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  booktitle = {Psychological Review},
  pages = {386-408},
  numpages = {23},
  keywords = {perceptron, neural networks, machine learning, artificial intelligence},
  location = {Cambridge, MA, USA},
  series = {PR '58}
}
@inproceedings{Rosenblatt1961,
  author = {Rosenblatt, Frank},
  title = {Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
  year = {1961},
  isbn = {9780262181114},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  booktitle = {Neural Networks},
  pages = {386-408},
  numpages = {23},
  keywords = {perceptron, neural networks, machine learning, artificial intelligence},
  location = {Cambridge, MA, USA},
  series = {NN '61}
}
@inproceedings{LeCun1998,
  author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  title = {Gradient-based learning applied to document recognition},
  year = {1998},
  isbn = {1558605529},
  publisher = {IEEE},
  address = {Washington, DC, USA},
  booktitle = {Proceedings of the IEEE},
  pages = {2278-2324},
  numpages = {47},
  keywords = {Convolutional neural networks, document recognition, image recognition, neural networks, pattern recognition},
  location = {Washington, DC, USA},
  series = {IEEE '98}
}
@inproceedings{Krizhevsky2012,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2012},
  isbn = {9781450319895},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {1097-1105},
  numpages = {9},
  keywords = {Convolutional neural networks, deep learning, image classification, image recognition, neural networks},
  location = {Lake Tahoe, Nevada, USA},
  series = {NIPS '12}
}
@article{Hochreiter1997,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}
@article{Hochreiter1991,
      author = {Hochreiter, Sepp},
      title = "{Untersuchungen zu dynamischen neuronalen Netzen}",
      journal = {Diploma Thesis},
      year = {1991},
      month = {July},
      abstract = "{The thesis investigates the application of dynamic neural networks to time-dependent problems. It is found that standard gradient-based learning algorithms, such as backpropagation, fail to provide an adequate way of adjusting the weights in recurrent networks. A major reason is the vanishing gradient problem, which causes the gradients to be small and, therefore, learning to be slow. To overcome this problem, the Long Short-Term Memory (LSTM) architecture is proposed. LSTM has a more complex structure than standard neural networks, but provides a way of determining the error gradient in a more direct way. The architecture is evaluated on a number of benchmark problems, such as predicting chaotic time series, and on some real-world tasks, such as speech recognition and handwriting recognition.}",
      url = {http://www.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf},
   }
@article{Devlin2019,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2019},
  isbn = {9781450362856},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
  journal = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {4171-4186},
  numpages = {16},
  keywords = {BERT, transformers, language understanding, pre-training},
  location = {Minneapolis, MN, USA},
  series = {NAACL-HLT '19}
}
@article{Rumelhart1986,
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title = {Learning representations by back-propagating errors},
  year = {1986},
  isbn = {0934613448},
  publisher = {Nature Publishing Group},
  address = {London, UK},
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533-536},
  numpages = {4},
  keywords = {backpropagation, neural networks, machine learning, artificial intelligence},
  location = {London, UK},
  series = {Nature '86}
}
@article{Chen2021,
  author = {Chen, Yen-Chun and Liu, Yung-Sung and Kira, Zsolt and AlRegib, Ghassan},
  title = {Pre-trained Image Processing Transformer},
  year = {2021},
  isbn = {9781450383325},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  journal = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages = {3530-3539},
  numpages = {10},
  keywords = {image processing, transformers, pre-training},
  location = {Virtual Event, China},
  series = {MM '21}
}
@article{Dosovitskiy2021,
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Michael and Heigold, Georg and Gelly, Sylvain and Houlsby, Neil},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year = {2021},
  isbn = {9781450383325},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  journal = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages = {3530-3539},
  numpages = {10},
  keywords = {image recognition, transformers, pre-training},
  location = {Virtual Event, China},
  series = {MM '21}
}
@ARTICLE{Han2023,
  author={Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={A Survey on Vision Transformer},
  year={2023},
  volume={45},
  number={1},
  pages={87-110},
  keywords={Transformers;Task analysis;Encoding;Computer vision;Computational modeling;Visualization;Object detection;Computer vision;high-level vision;low-level vision;self-attention;transformer;video},
  doi={10.1109/TPAMI.2022.3152247}
}
@misc{Zhao2023,
      title={A Survey of Large Language Models},
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18223},
}
@article{Radford2021,
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Hesse, Emma and Wang, Heewoo and Dorado, Jonathan and Park, Myungwon and Foo, John and Steorts, Ethan and Sutskever, Ilya},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  year = {2021},
  isbn = {9781450383325},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  journal = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages = {3530-3539},
  numpages = {10},
  keywords = {image recognition, transformers, pre-training},
  location = {Virtual Event, China},
  series = {MM '21}
}
@misc{Gemini2022,
      title={Gemini: A Family of Highly Capable Multimodal Models},
      author={Gemini Team et al.},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805},
}
@misc{Gemini2024,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
      author={Gemini Team et al.},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530},
}
@misc{Ouyang2022,
      title={Training language models to follow instructions with human feedback},
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155},
}
@article{Bommasani2021,
    title={On the Opportunities and Risks of Foundation Models},
    author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
    journal={ArXiv},
    year={2021},
    url={https://crfm.stanford.edu/assets/report.pdf}
}
@INPROCEEDINGS{Zellers2019,
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={From Recognition to Cognition: Visual Commonsense Reasoning},
  year={2019},
  volume={},
  number={},
  pages={6713-6724},
  keywords={Visualization;Grounding;Transforms;Motion pictures;Pattern recognition;Object recognition;Task analysis;Vision + Language;Recognition: Detection;Categorization;Retrieval;Scene Analysis and Understanding;Visual Reasonin},
  doi={10.1109/CVPR.2019.00688}
}
@article{Martin2023,
  author    = {Roberto Mart\'in-Mart\'in and Mihir Patel and Hamid Rezatofighi and et al.},
  title     = {{JRDB: A Dataset and Benchmark of Egocentric Robot Visual Perception of Humans in Built Environments}},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2023},
  volume    = {45},
  number    = {6},
  pages     = {6748--6765},
  doi       = {10.1109/TPAMI.2021.3070543}
}
@inproceedings{Alayrac2022,
    title={Flamingo: a Visual Language Model for Few-Shot Learning},
    author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=EbMuimAbPbs}
}
@misc{TableNet2020,
      title={TableNet: Deep Learning model for end-to-end Table detection and Tabular data extraction from Scanned Document Images},
      author={Shubham Paliwal and Vishwanath D and Rohit Rahul and Monika Sharma and Lovekesh Vig},
      year={2020},
      eprint={2001.01469},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2001.01469},
}
@INPROCEEDINGS{Gilani2017,
  author={Gilani, Azka and Qasim, Shah Rukh and Malik, Imran and Shafait, Faisal},
  booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
  title={Table Detection Using Deep Learning},
  year={2017},
  volume={01},
  number={},
  pages={771-776},
  keywords={Layout;Machine learning;Proposals;Text analysis;Portable document format;Transforms;Optical character recognition software;Table Detection;Document Analysis;Deep Learning},
  doi={10.1109/ICDAR.2017.131}
}
@inproceedings{Shafait2010,
author = {Shafait, Faisal and Smith, Ray},
title = {Table detection in heterogeneous documents},
year = {2010},
isbn = {9781605587738},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1815330.1815339},
doi = {10.1145/1815330.1815339},
abstract = {Detecting tables in document images is important since not only do tables contain important information, but also most of the layout analysis methods fail in the presence of tables in the document image. Existing approaches for table detection mainly focus on detecting tables in single columns of text and do not work reliably on documents with varying layouts. This paper presents a practical algorithm for table detection that works with a high accuracy on documents with varying layouts (company reports, newspaper articles, magazine pages, ...). An open source implementation of the algorithm is provided as part of the Tesseract OCR engine. Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system.},
booktitle = {Proceedings of the 9th IAPR International Workshop on Document Analysis Systems},
pages = {65–72},
numpages = {8},
keywords = {table detection, page segmentation, document analysis},
location = {Boston, Massachusetts, USA},
series = {DAS '10}
}
